{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from limit_states.g11d_electric import g11d_electric\n",
    "from methods.sghmc import *\n",
    "from utils.data import get_dataloader\n",
    "from active_training.active_train import ActiveTrain\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine humming\n",
      "ref PF: 0.000201 B: 3.5387673355471683\n"
     ]
    }
   ],
   "source": [
    "lstate = g11d_electric()\n",
    "act_train = ActiveTrain()\n",
    "pf, beta, _,_, y_mc_test = lstate.monte_carlo_estimate(1e6)\n",
    "y_max = np.max(y_mc_test)   #to normalise the output for training\n",
    "print('ref PF:', pf, 'B:',beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:  100 "
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "d:\\Benchmark_BNNs_StructuralReliability\\methods\\sghmc.py:307: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at C:\\actions-runner\\_work\\pytorch\\pytorch\\builder\\windows\\pytorch\\torch\\csrc\\utils\\python_arg_parser.cpp:1519.)\n",
      "  d_p.add_(weight_decay, p.data)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train_loss = 0.141485, pf_ref 0.000206 pf_surrogate  0.015595000237226486\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Administrator\\AppData\\Local\\Temp\\ipykernel_6020\\1796630144.py:25: UserWarning: To copy construct from a tensor, it is recommended to use sourceTensor.clone().detach() or sourceTensor.clone().detach().requires_grad_(True), rather than torch.tensor(sourceTensor).\n",
      "  x_train = torch.tensor(x_norm, dtype=torch.float32).view(-1,11)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:  105 train_loss = 0.086556, pf_ref 0.000195 pf_surrogate  0.012362999841570854\n",
      "Samples:  110 train_loss = 0.092514, pf_ref 0.00023 pf_surrogate  0.0050650001503527164\n",
      "Samples:  115 train_loss = 0.145630, pf_ref 0.000219 pf_surrogate  0.026112999767065048\n",
      "Samples:  120 train_loss = 0.074814, pf_ref 0.000209 pf_surrogate  0.004612999968230724\n",
      "Samples:  125 train_loss = 0.115071, pf_ref 0.000207 pf_surrogate  0.0007200000109151006\n",
      "Samples:  130 train_loss = 0.086130, pf_ref 0.000201 pf_surrogate  0.002047999994829297\n",
      "Samples:  135 train_loss = 0.044238, pf_ref 0.0002 pf_surrogate  1.9999999949504854e-06\n",
      "Samples:  140 train_loss = 0.101031, pf_ref 0.000197 pf_surrogate  3.999999989900971e-06\n",
      "Samples:  145 train_loss = 0.097415, pf_ref 0.000222 pf_surrogate  0.0030580000020563602\n",
      "Samples:  150 train_loss = 0.051092, pf_ref 0.000182 pf_surrogate  0.0\n",
      "Samples:  155 train_loss = 0.066746, pf_ref 0.000204 pf_surrogate  0.0005559999844990671\n",
      "Samples:  160 train_loss = 0.133464, pf_ref 0.000197 pf_surrogate  0.0029219998978078365\n",
      "Samples:  165 train_loss = 0.059423, pf_ref 0.000212 pf_surrogate  0.010382999666035175\n",
      "Samples:  170 train_loss = 0.055713, pf_ref 0.000191 pf_surrogate  0.0\n",
      "Samples:  175 train_loss = 0.058508, pf_ref 0.000226 pf_surrogate  0.00022699999681208283\n",
      "Samples:  180 train_loss = 0.043435, pf_ref 0.000186 pf_surrogate  0.0\n",
      "Samples:  185 train_loss = 0.065867, pf_ref 0.000186 pf_surrogate  1.8000000636675395e-05\n",
      "Samples:  190 train_loss = 0.066724, pf_ref 0.000206 pf_surrogate  0.0005460000247694552\n",
      "Samples:  195 train_loss = 0.049430, pf_ref 0.000198 pf_surrogate  0.0001829999964684248\n",
      "End training\n"
     ]
    }
   ],
   "source": [
    "#Passive training\n",
    "passive_samples = 100 \n",
    "x_norm, x_scaled, y_scaled = lstate.get_doe_points(n_samples=passive_samples, method='lhs')\n",
    "batch_size = 64\n",
    "\n",
    "#network config\n",
    "width, layers = 20, 2\n",
    "\n",
    "# Active training\n",
    "use_cuda = torch.cuda.is_available()\n",
    "n_active_ep = 20\n",
    "active_points = 5\n",
    "n_passive_ep = 100\n",
    "mcs_samples = int(1e6)\n",
    "\n",
    "burn_in = 20   #How many epochs to burn in for?. Default: 20.\n",
    "sim_steps = 2   #How many epochs pass between saving samples. Default: 2.\n",
    "N_saves=10\n",
    "resample_its = 50\n",
    "resample_prior_its = 15\n",
    "re_burn = 1e8\n",
    "\n",
    "for act_ep in range(n_active_ep):\n",
    "\n",
    "    x_train = torch.tensor(x_norm, dtype=torch.float32).view(-1,11)\n",
    "    y_train = torch.tensor(y_scaled/y_max, dtype=torch.float32).view(-1,1)  #normalised output\n",
    "    \n",
    "    print('Samples: ', x_train.shape[0], end=\" \")\n",
    "    \n",
    "    train_loader, _ = get_dataloader(x_train, y_train, lstate.input_dim, lstate.output_dim, train_test_split=1.0, batch_size=batch_size)\n",
    "\n",
    "    net = BNN_SGHMC(N_train=len(x_train), input_dim=lstate.input_dim, width=width, depth=layers, output_dim=lstate.output_dim, \n",
    "                lr=1e-2, cuda=use_cuda, grad_std_mul=10)\n",
    "\n",
    "    net.train(train_loader, epoch=n_passive_ep, burn_in=burn_in, re_burn = re_burn , \n",
    "                resample_its=resample_its, resample_prior_its = resample_prior_its, \n",
    "                sim_steps = sim_steps, N_saves=N_saves, verbose=0)\n",
    "        \n",
    "    Pf_ref, B_ref, x_mc_norm, x_mc_scaled, _ = lstate.monte_carlo_estimate(mcs_samples)\n",
    "    X_uq = torch.tensor(x_mc_norm, dtype=torch.float32)\n",
    "\n",
    "    print('pf_ref', Pf_ref, end=\" \")\n",
    "\n",
    "    y_bnn_pred = net.sample_predict(X_uq, Nsamples=N_saves)\n",
    "    y_bnn_mean = torch.mean(y_bnn_pred, 0)\n",
    "    y_bnn_std = torch.std(y_bnn_pred, 0)\n",
    "\n",
    "    print('pf_surrogate ' , (((y_bnn_mean<0).sum()) / torch.tensor(mcs_samples)).item() )\n",
    "\n",
    "    x_norm = act_train.get_active_points(x_train, X_uq, y_bnn_pred.view(-1, N_saves), active_points)\n",
    "    x_scaled = lstate.isoprob_transform(x_norm, lstate.marginals)\n",
    "    y_scaled = lstate.eval_lstate(x_scaled)\n",
    "\n",
    "print('End training')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
