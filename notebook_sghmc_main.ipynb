{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from limit_states.g11d_electric import g11d_electric\n",
    "from methods.sghmc import *\n",
    "from utils.data import get_dataloader\n",
    "from active_training.active_train import ActiveTrain\n",
    "from tqdm import tqdm\n",
    "\n",
    "import torch.utils.data as data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "engine humming\n",
      "ref PF: 0.000207 B: 3.53099472767886\n"
     ]
    }
   ],
   "source": [
    "lstate = g11d_electric()\n",
    "act_train = ActiveTrain()\n",
    "pf, beta, _,_, y_mc_test = lstate.monte_carlo_estimate(1e6)\n",
    "y_max = np.max(y_mc_test)   #to normalise the output for training\n",
    "print('ref PF:', pf, 'B:',beta)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Passive training\n",
    "x_norm, x_scaled, y_scaled = lstate.get_doe_points(n_samples=100, method='lhs')\n",
    "\n",
    "x_train = torch.tensor(x_norm, dtype=torch.float32).view(-1,11)\n",
    "y_train = torch.tensor(y_scaled/y_max, dtype=torch.float32).view(-1,1)  #normalised output"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Samples:  100\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "  0%|          | 0/50 [00:00<?, ?it/s]/Users/jonathan/Documents/SGHMC/Benchmark_BNNs_StructuralReliability/methods/sghmc.py:273: UserWarning: This overload of add_ is deprecated:\n",
      "\tadd_(Number alpha, Tensor other)\n",
      "Consider using one of the following signatures instead:\n",
      "\tadd_(Tensor other, *, Number alpha) (Triggered internally at /Users/runner/work/pytorch/pytorch/pytorch/torch/csrc/utils/python_arg_parser.cpp:1485.)\n",
      "  d_p.add_(weight_decay, p.data)\n",
      "100%|██████████| 50/50 [00:00<00:00, 241.06it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.0002\n",
      "pf_surrogate  0.02377999946475029\n",
      "Samples:  105\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 283.11it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00025\n",
      "pf_surrogate  0.0\n",
      "Samples:  110\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 281.35it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00017\n",
      "pf_surrogate  0.0\n",
      "Samples:  115\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 279.37it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00019\n",
      "pf_surrogate  0.010320000350475311\n",
      "Samples:  120\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 278.48it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00019\n",
      "pf_surrogate  0.0\n",
      "Samples:  125\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 232.19it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00019\n",
      "pf_surrogate  0.0\n",
      "Samples:  130\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 238.31it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.0002\n",
      "pf_surrogate  0.0\n",
      "Samples:  135\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 235.91it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00021\n",
      "pf_surrogate  0.0\n",
      "Samples:  140\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 222.18it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.0002\n",
      "pf_surrogate  0.0\n",
      "Samples:  145\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 232.30it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00021\n",
      "pf_surrogate  0.0\n",
      "Samples:  150\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 235.74it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00021\n",
      "pf_surrogate  0.0\n",
      "Samples:  155\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 228.72it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00019\n",
      "pf_surrogate  0.0\n",
      "Samples:  160\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 212.05it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00021\n",
      "pf_surrogate  0.0\n",
      "Samples:  165\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 188.89it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00017\n",
      "pf_surrogate  0.0\n",
      "Samples:  170\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 197.88it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00014\n",
      "pf_surrogate  0.0\n",
      "Samples:  175\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 197.84it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00018\n",
      "pf_surrogate  0.0002099999983329326\n",
      "Samples:  180\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 196.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00016\n",
      "pf_surrogate  0.0\n",
      "Samples:  185\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 196.70it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00023\n",
      "pf_surrogate  9.999999747378752e-06\n",
      "Samples:  190\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 195.93it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00019\n",
      "pf_surrogate  0.0\n",
      "Samples:  195\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 50/50 [00:00<00:00, 159.07it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pf_ref 0.00025\n",
      "pf_surrogate  0.0\n",
      "End training\n"
     ]
    }
   ],
   "source": [
    "# Active training\n",
    "use_cuda = torch.cuda.is_available()\n",
    "# for i in tqdm(range(epoch, nb_epochs)):\n",
    "\n",
    "n_active_ep = 20\n",
    "active_points = 5\n",
    "mcs_samples = int(1e5)\n",
    "it_count = 0\n",
    "\n",
    "burn_in = 10   #How many epochs to burn in for?. Default: 20.\n",
    "sim_steps = 2   #How many epochs pass between saving samples. Default: 2.\n",
    "N_saves=10\n",
    "resample_its = 50\n",
    "resample_prior_its = 15\n",
    "re_burn = 1e8\n",
    "\n",
    "n_passive_ep = 50\n",
    "\n",
    "for act_ep in range(n_active_ep):\n",
    "    \n",
    "    print('Samples: ', x_train.shape[0])\n",
    "    \n",
    "    train_loader, _ = get_dataloader(x_train, y_train, lstate.input_dim, lstate.output_dim, train_test_split=1.0, batch_size=32)\n",
    "\n",
    "    net = BNN_SGHMC(N_train=len(x_train), input_dim=11, width=20, depth=2, output_dim=1, \n",
    "              lr=1e-2, cuda=use_cuda, grad_std_mul=10)\n",
    "\n",
    "    cost_train = np.zeros(n_passive_ep)\n",
    "    for pass_ep in tqdm(range(0, n_passive_ep)):\n",
    "        net.set_mode_train(True)\n",
    "        nb_samples = 0\n",
    "\n",
    "        # passive training with current dataset\n",
    "        for x, y in train_loader:\n",
    "            \n",
    "            cost_pred = net.fit(x, y, burn_in=(pass_ep % re_burn < burn_in),\n",
    "                                    resample_momentum=(it_count % resample_its == 0),\n",
    "                                    resample_prior=(it_count % resample_prior_its == 0))\n",
    "            it_count += 1\n",
    "            cost_train[pass_ep] += cost_pred\n",
    "            nb_samples += len(x)\n",
    "        \n",
    "        cost_train[pass_ep] /= nb_samples\n",
    "        net.update_lr(pass_ep)\n",
    "\n",
    "        if pass_ep % re_burn >= burn_in and pass_ep % sim_steps == 0:\n",
    "            net.save_sampled_net(max_samples=N_saves)\n",
    "\n",
    "    Pf_ref, B_ref, x_mc_norm, x_mc_scaled, _ = lstate.monte_carlo_estimate(mcs_samples)\n",
    "    X_uq = torch.tensor(x_mc_norm, dtype=torch.float32)\n",
    "\n",
    "    print('pf_ref', Pf_ref)\n",
    "\n",
    "    y_bnn_pred = net.sample_predict(X_uq, Nsamples=N_saves)\n",
    "    y_bnn_mean = torch.mean(y_bnn_pred*y_max, 0)\n",
    "    y_bnn_std = torch.std(y_bnn_pred*y_max , 0)\n",
    "\n",
    "    print('pf_surrogate ' , (((y_bnn_mean<0).sum()) / torch.tensor(mcs_samples)).item() )\n",
    "\n",
    "    U_f = np.abs(y_bnn_mean) / y_bnn_std\n",
    "\n",
    "    U_min_args = np.argsort(U_f.reshape(-1))     #ordering arguments from min to max\n",
    "    X_new = x_mc_scaled[U_min_args[:active_points]]  #choosing a given number of MC samples from the minimum U values\n",
    "    X_new_norm = X_uq[U_min_args[:active_points]]  #choosing a given number of MC samples from the minimum U values\n",
    "    x_train = torch.cat( (x_train, X_new_norm))\n",
    "\n",
    "    Y_new_norm = lstate.eval_lstate(X_new) / y_max\n",
    "    Y_new_norm = torch.tensor(Y_new_norm, dtype=torch.float32).view(active_points, 1)\n",
    "    y_train = torch.cat( (y_train, Y_new_norm))\n",
    "\n",
    "print('End training')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deeplearning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
