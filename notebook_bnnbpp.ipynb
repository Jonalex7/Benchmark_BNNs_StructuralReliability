{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "\n",
    "#for this notebook and probably main file\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "from utils.data import get_dataloader"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<matplotlib.legend.Legend at 0x244d2293190>"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "data": {
      "image/png": "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",
      "text/plain": [
       "<Figure size 1000x500 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "def function(x):\n",
    "    return -(torch.sin(x*7)+torch.cos(14*x))*(x**2)*torch.exp(-4*x)*100\n",
    "\n",
    "#range for sampling\n",
    "xmin=0.2\n",
    "xmax=1.00\n",
    "points_plot=1000                              #resolution to plot the ground truth \n",
    "x_plot=torch.linspace(xmin, xmax, points_plot) \n",
    "y_plot=function(x_plot)\n",
    "\n",
    "fig = plt.figure(figsize=(10,5))\n",
    "ax=fig.add_axes([0,0,1,1])\n",
    "ax.plot(x_plot, y_plot,label=r\"Ground truth\",linestyle='dashed',linewidth=0.5,color='k')\n",
    "# ax.scatter(t2,PC,label=r\"Predictions\",color='g',s=10)\n",
    "# plt.scatter(xn, M, label=\"Observations\",marker=\"x\",color='r',s=200)\n",
    "plt.legend(prop={'size': 10})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(torch.Size([32, 1]), torch.Size([32, 1]))"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_samples = 500                   \n",
    "x_data=np.random.uniform(xmin, xmax, training_samples)\n",
    "x_data = torch.from_numpy(x_data.astype(np.float32)).view(-1,1)\n",
    "y_data=function(x_data)\n",
    "train_loader, _ = get_dataloader(x_data, y_data, 1, 1, train_test_split=1.0, batch_size=32)\n",
    "\n",
    "x_batch,y_batch = next(iter(train_loader))  #to fix the x value in the reconstruction training\n",
    "x_batch.shape, y_batch.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BayesianLayer(nn.Module):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super(BayesianLayer, self).__init__()\n",
    "        self.in_features = in_features\n",
    "        self.out_features = out_features\n",
    "        self.weight_mu = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.weight_rho = nn.Parameter(torch.Tensor(out_features, in_features))\n",
    "        self.bias_mu = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.bias_rho = nn.Parameter(torch.Tensor(out_features))\n",
    "        self.reset_parameters()\n",
    "        \n",
    "    def reset_parameters(self):\n",
    "        nn.init.kaiming_normal_(self.weight_mu, mode='fan_in', nonlinearity='relu')\n",
    "        nn.init.constant_(self.weight_rho, -6)  # Initialize log variance to a small value\n",
    "        nn.init.zeros_(self.bias_mu)\n",
    "        nn.init.constant_(self.bias_rho, -6)\n",
    "\n",
    "    # def reset_parameters(self):\n",
    "    #     # Initialize weights and biases with normal distribution\n",
    "    #     nn.init.normal_(self.weight_mu, mean=0, std=0.1)\n",
    "    #     nn.init.normal_(self.weight_rho, mean=-3, std=0.1)\n",
    "    #     nn.init.normal_(self.bias_mu, mean=0, std=0.1)\n",
    "    #     nn.init.normal_(self.bias_rho, mean=-3, std=0.1)\n",
    "\n",
    "    def forward(self, x):\n",
    "        weight_sigma = torch.log1p(torch.exp(self.weight_rho))\n",
    "        bias_sigma = torch.log1p(torch.exp(self.bias_rho))\n",
    "        epsilon_weight = Variable(torch.randn_like(weight_sigma))\n",
    "        epsilon_bias = Variable(torch.randn_like(bias_sigma))\n",
    "        weight = self.weight_mu + weight_sigma * epsilon_weight\n",
    "        bias = self.bias_mu + bias_sigma * epsilon_bias\n",
    "        output = F.linear(x, weight, bias)\n",
    "        return output, weight, bias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN_BPP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(BNN_BPP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define the Bayesian neural network architecture based any number of hidden_sizes\n",
    "        self.fc_mu_1 = BayesianLayer(input_size, hidden_sizes)\n",
    "        self.fc_mu_2 = BayesianLayer(hidden_sizes, output_size)\n",
    "\n",
    "        # def variational_loss(self, output_mu, output_log_var, target):\n",
    "        #     recon_loss = F.mse_loss(output_mu, target, reduction='mean')\n",
    "        #     kl_divergence = -0.5 * torch.sum(1 + output_log_var - output_mu.pow(2) - output_log_var.exp())\n",
    "        #     return recon_loss , kl_divergence\n",
    "        \n",
    "    def forward(self, x):\n",
    "        hidden_out, _ , _ = self.fc_mu_1(x)\n",
    "        hidden_out  = F.relu(hidden_out)\n",
    "        output , _ , _  = self.fc_mu_2(hidden_out)\n",
    "        return output\n",
    "\n",
    "    def train(self, train_loader, num_epochs=10, lr=1e-3, verbose=0):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, targets in train_loader:\n",
    "                optimizer.zero_grad()\n",
    "                output_mu = self(inputs)\n",
    "                recon_loss = F.mse_loss(output_mu, targets, reduction='mean')\n",
    "                kl_loss = 0\n",
    "\n",
    "                for layer in self.children():\n",
    "                    if isinstance(layer, BayesianLayer):\n",
    "                        weight_mu = layer.weight_mu\n",
    "                        weight_rho = layer.weight_rho\n",
    "                        bias_mu = layer.bias_mu\n",
    "                        bias_rho = layer.bias_rho\n",
    "                        # Compute KL divergence for weights\n",
    "                        kl_loss += 0.5 * (torch.sum(weight_mu**2) + torch.sum(torch.log1p(torch.exp(weight_rho)) - weight_rho) - weight_mu.numel())\n",
    "                        # Compute KL divergence for biases\n",
    "                        kl_loss += 0.5 * (torch.sum(bias_mu**2) + torch.sum(torch.log1p(torch.exp(bias_rho)) - bias_rho) - bias_mu.numel())\n",
    "                # loss_recons, kl_loss = self.variational_loss(output_mu, output_log_var, targets)\n",
    "                kl_loss = kl_loss / 1000\n",
    "                loss = recon_loss + (kl_loss)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if not verbose == 0: \n",
    "                # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss MSE: {recon_loss}, Loss KL: {kl_loss}')\n",
    "            \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], total training loss: {loss.item()}')             \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "BNN_BPP(\n",
      "  (fc_mu_1): BayesianLayer()\n",
      "  (fc_mu_2): BayesianLayer()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "# Example usage\n",
    "input_size = 1\n",
    "hidden_sizes = 20\n",
    "output_size = 1\n",
    "\n",
    "net = BNN_BPP(input_size, hidden_sizes, output_size)\n",
    "print(net)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/1000], Loss MSE: 6.665107727050781, Loss KL: 0.16874055564403534\n",
      "Epoch [2/1000], Loss MSE: 9.675190925598145, Loss KL: 0.1680876463651657\n",
      "Epoch [3/1000], Loss MSE: 9.161306381225586, Loss KL: 0.16744329035282135\n",
      "Epoch [4/1000], Loss MSE: 6.974194526672363, Loss KL: 0.16680823266506195\n",
      "Epoch [5/1000], Loss MSE: 11.365663528442383, Loss KL: 0.16618771851062775\n",
      "Epoch [6/1000], Loss MSE: 5.871662139892578, Loss KL: 0.1655709445476532\n",
      "Epoch [7/1000], Loss MSE: 11.155831336975098, Loss KL: 0.16494813561439514\n",
      "Epoch [8/1000], Loss MSE: 10.09666919708252, Loss KL: 0.1643330603837967\n",
      "Epoch [9/1000], Loss MSE: 7.832395076751709, Loss KL: 0.16372941434383392\n",
      "Epoch [10/1000], Loss MSE: 8.903972625732422, Loss KL: 0.16311883926391602\n",
      "Epoch [11/1000], Loss MSE: 9.400113105773926, Loss KL: 0.16252082586288452\n",
      "Epoch [12/1000], Loss MSE: 10.231191635131836, Loss KL: 0.16193152964115143\n",
      "Epoch [13/1000], Loss MSE: 8.488205909729004, Loss KL: 0.16134533286094666\n",
      "Epoch [14/1000], Loss MSE: 10.319299697875977, Loss KL: 0.16074903309345245\n",
      "Epoch [15/1000], Loss MSE: 9.693001747131348, Loss KL: 0.1601782739162445\n",
      "Epoch [16/1000], Loss MSE: 10.184885025024414, Loss KL: 0.15960679948329926\n",
      "Epoch [17/1000], Loss MSE: 9.573156356811523, Loss KL: 0.15904130041599274\n",
      "Epoch [18/1000], Loss MSE: 6.627442836761475, Loss KL: 0.15847375988960266\n",
      "Epoch [19/1000], Loss MSE: 5.3283586502075195, Loss KL: 0.15789923071861267\n",
      "Epoch [20/1000], Loss MSE: 5.887850761413574, Loss KL: 0.15731948614120483\n",
      "Epoch [21/1000], Loss MSE: 10.351163864135742, Loss KL: 0.15676796436309814\n",
      "Epoch [22/1000], Loss MSE: 7.907549858093262, Loss KL: 0.1562241166830063\n",
      "Epoch [23/1000], Loss MSE: 5.807491779327393, Loss KL: 0.15568314492702484\n",
      "Epoch [24/1000], Loss MSE: 8.924782752990723, Loss KL: 0.15515026450157166\n",
      "Epoch [25/1000], Loss MSE: 9.863481521606445, Loss KL: 0.15460507571697235\n",
      "Epoch [26/1000], Loss MSE: 9.128473281860352, Loss KL: 0.15406838059425354\n",
      "Epoch [27/1000], Loss MSE: 8.794867515563965, Loss KL: 0.15354414284229279\n",
      "Epoch [28/1000], Loss MSE: 11.17892837524414, Loss KL: 0.15302729606628418\n",
      "Epoch [29/1000], Loss MSE: 6.500199794769287, Loss KL: 0.1525196135044098\n",
      "Epoch [30/1000], Loss MSE: 10.155558586120605, Loss KL: 0.15201841294765472\n",
      "Epoch [31/1000], Loss MSE: 8.095487594604492, Loss KL: 0.1515231877565384\n",
      "Epoch [32/1000], Loss MSE: 6.4911065101623535, Loss KL: 0.1510390043258667\n",
      "Epoch [33/1000], Loss MSE: 7.4129204750061035, Loss KL: 0.15055693686008453\n",
      "Epoch [34/1000], Loss MSE: 10.045804977416992, Loss KL: 0.15007668733596802\n",
      "Epoch [35/1000], Loss MSE: 10.756092071533203, Loss KL: 0.149614617228508\n",
      "Epoch [36/1000], Loss MSE: 8.523396492004395, Loss KL: 0.14915747940540314\n",
      "Epoch [37/1000], Loss MSE: 7.039040565490723, Loss KL: 0.1487026959657669\n",
      "Epoch [38/1000], Loss MSE: 10.101179122924805, Loss KL: 0.14823995530605316\n",
      "Epoch [39/1000], Loss MSE: 8.379373550415039, Loss KL: 0.14779341220855713\n",
      "Epoch [40/1000], Loss MSE: 11.069841384887695, Loss KL: 0.14736205339431763\n",
      "Epoch [41/1000], Loss MSE: 13.511320114135742, Loss KL: 0.1469193994998932\n",
      "Epoch [42/1000], Loss MSE: 9.981500625610352, Loss KL: 0.14647498726844788\n",
      "Epoch [43/1000], Loss MSE: 8.713647842407227, Loss KL: 0.14603668451309204\n",
      "Epoch [44/1000], Loss MSE: 9.132561683654785, Loss KL: 0.14559166133403778\n",
      "Epoch [45/1000], Loss MSE: 4.756247520446777, Loss KL: 0.1451719105243683\n",
      "Epoch [46/1000], Loss MSE: 9.038118362426758, Loss KL: 0.14473916590213776\n",
      "Epoch [47/1000], Loss MSE: 10.86872386932373, Loss KL: 0.14430886507034302\n",
      "Epoch [48/1000], Loss MSE: 10.228947639465332, Loss KL: 0.14388127624988556\n",
      "Epoch [49/1000], Loss MSE: 9.095094680786133, Loss KL: 0.143464133143425\n",
      "Epoch [50/1000], Loss MSE: 5.16813325881958, Loss KL: 0.1430623084306717\n",
      "Epoch [51/1000], Loss MSE: 6.535012245178223, Loss KL: 0.14264734089374542\n",
      "Epoch [52/1000], Loss MSE: 12.104146957397461, Loss KL: 0.14222043752670288\n",
      "Epoch [53/1000], Loss MSE: 11.58445930480957, Loss KL: 0.14183259010314941\n",
      "Epoch [54/1000], Loss MSE: 7.196702480316162, Loss KL: 0.14142762124538422\n",
      "Epoch [55/1000], Loss MSE: 9.296570777893066, Loss KL: 0.14102211594581604\n",
      "Epoch [56/1000], Loss MSE: 10.772616386413574, Loss KL: 0.14062200486660004\n",
      "Epoch [57/1000], Loss MSE: 10.925554275512695, Loss KL: 0.14022308588027954\n",
      "Epoch [58/1000], Loss MSE: 7.854704856872559, Loss KL: 0.13981710374355316\n",
      "Epoch [59/1000], Loss MSE: 7.4289422035217285, Loss KL: 0.1394120752811432\n",
      "Epoch [60/1000], Loss MSE: 8.406373023986816, Loss KL: 0.1390112191438675\n",
      "Epoch [61/1000], Loss MSE: 8.638705253601074, Loss KL: 0.13863033056259155\n",
      "Epoch [62/1000], Loss MSE: 8.990291595458984, Loss KL: 0.13824035227298737\n",
      "Epoch [63/1000], Loss MSE: 6.412489414215088, Loss KL: 0.13785725831985474\n",
      "Epoch [64/1000], Loss MSE: 6.1540446281433105, Loss KL: 0.13749031722545624\n",
      "Epoch [65/1000], Loss MSE: 10.646102905273438, Loss KL: 0.13710980117321014\n",
      "Epoch [66/1000], Loss MSE: 11.524652481079102, Loss KL: 0.13672205805778503\n",
      "Epoch [67/1000], Loss MSE: 11.171281814575195, Loss KL: 0.13634152710437775\n",
      "Epoch [68/1000], Loss MSE: 7.479418754577637, Loss KL: 0.1359674036502838\n",
      "Epoch [69/1000], Loss MSE: 7.324957847595215, Loss KL: 0.13560126721858978\n",
      "Epoch [70/1000], Loss MSE: 9.63556957244873, Loss KL: 0.1352146863937378\n",
      "Epoch [71/1000], Loss MSE: 9.035455703735352, Loss KL: 0.13483570516109467\n",
      "Epoch [72/1000], Loss MSE: 9.77735424041748, Loss KL: 0.13445767760276794\n",
      "Epoch [73/1000], Loss MSE: 4.833485126495361, Loss KL: 0.13410323858261108\n",
      "Epoch [74/1000], Loss MSE: 8.657804489135742, Loss KL: 0.13373954594135284\n",
      "Epoch [75/1000], Loss MSE: 6.4212470054626465, Loss KL: 0.1333814561367035\n",
      "Epoch [76/1000], Loss MSE: 7.264601230621338, Loss KL: 0.13302642107009888\n",
      "Epoch [77/1000], Loss MSE: 10.023651123046875, Loss KL: 0.13267257809638977\n",
      "Epoch [78/1000], Loss MSE: 9.161786079406738, Loss KL: 0.13231894373893738\n",
      "Epoch [79/1000], Loss MSE: 7.043231964111328, Loss KL: 0.1319608837366104\n",
      "Epoch [80/1000], Loss MSE: 7.706055641174316, Loss KL: 0.1316390484571457\n",
      "Epoch [81/1000], Loss MSE: 13.244142532348633, Loss KL: 0.13127577304840088\n",
      "Epoch [82/1000], Loss MSE: 4.827145099639893, Loss KL: 0.13093172013759613\n",
      "Epoch [83/1000], Loss MSE: 7.8275604248046875, Loss KL: 0.13058018684387207\n",
      "Epoch [84/1000], Loss MSE: 10.623133659362793, Loss KL: 0.13023649156093597\n",
      "Epoch [85/1000], Loss MSE: 9.383949279785156, Loss KL: 0.1298968642950058\n",
      "Epoch [86/1000], Loss MSE: 8.217350959777832, Loss KL: 0.1295732706785202\n",
      "Epoch [87/1000], Loss MSE: 9.896113395690918, Loss KL: 0.12925444543361664\n",
      "Epoch [88/1000], Loss MSE: 10.464972496032715, Loss KL: 0.1289406567811966\n",
      "Epoch [89/1000], Loss MSE: 9.529916763305664, Loss KL: 0.1286373734474182\n",
      "Epoch [90/1000], Loss MSE: 7.701525688171387, Loss KL: 0.12833403050899506\n",
      "Epoch [91/1000], Loss MSE: 7.473653316497803, Loss KL: 0.1280284970998764\n",
      "Epoch [92/1000], Loss MSE: 7.883615016937256, Loss KL: 0.12774676084518433\n",
      "Epoch [93/1000], Loss MSE: 5.986865997314453, Loss KL: 0.12743748724460602\n",
      "Epoch [94/1000], Loss MSE: 8.466288566589355, Loss KL: 0.12713097035884857\n",
      "Epoch [95/1000], Loss MSE: 6.377401828765869, Loss KL: 0.12682323157787323\n",
      "Epoch [96/1000], Loss MSE: 8.367902755737305, Loss KL: 0.12655705213546753\n",
      "Epoch [97/1000], Loss MSE: 7.754149436950684, Loss KL: 0.12626977264881134\n",
      "Epoch [98/1000], Loss MSE: 8.297325134277344, Loss KL: 0.12599730491638184\n",
      "Epoch [99/1000], Loss MSE: 8.86646842956543, Loss KL: 0.12570670247077942\n",
      "Epoch [100/1000], Loss MSE: 8.32974624633789, Loss KL: 0.12542173266410828\n",
      "Epoch [101/1000], Loss MSE: 8.223470687866211, Loss KL: 0.125144362449646\n",
      "Epoch [102/1000], Loss MSE: 7.995381832122803, Loss KL: 0.12488656491041183\n",
      "Epoch [103/1000], Loss MSE: 6.862744331359863, Loss KL: 0.12462165951728821\n",
      "Epoch [104/1000], Loss MSE: 12.118467330932617, Loss KL: 0.12432420998811722\n",
      "Epoch [105/1000], Loss MSE: 7.278526306152344, Loss KL: 0.12405243515968323\n",
      "Epoch [106/1000], Loss MSE: 12.430317878723145, Loss KL: 0.12380601465702057\n",
      "Epoch [107/1000], Loss MSE: 6.466846466064453, Loss KL: 0.12352738529443741\n",
      "Epoch [108/1000], Loss MSE: 7.166819095611572, Loss KL: 0.12324340641498566\n",
      "Epoch [109/1000], Loss MSE: 10.150411605834961, Loss KL: 0.12295689433813095\n",
      "Epoch [110/1000], Loss MSE: 9.290685653686523, Loss KL: 0.12269975244998932\n",
      "Epoch [111/1000], Loss MSE: 11.160120010375977, Loss KL: 0.12241558730602264\n",
      "Epoch [112/1000], Loss MSE: 10.627555847167969, Loss KL: 0.1221364215016365\n",
      "Epoch [113/1000], Loss MSE: 9.121908187866211, Loss KL: 0.1218826174736023\n",
      "Epoch [114/1000], Loss MSE: 4.949375629425049, Loss KL: 0.12163487076759338\n",
      "Epoch [115/1000], Loss MSE: 8.186838150024414, Loss KL: 0.12133877724409103\n",
      "Epoch [116/1000], Loss MSE: 8.816781997680664, Loss KL: 0.12108955532312393\n",
      "Epoch [117/1000], Loss MSE: 8.613364219665527, Loss KL: 0.12084940820932388\n",
      "Epoch [118/1000], Loss MSE: 7.859793663024902, Loss KL: 0.12058716267347336\n",
      "Epoch [119/1000], Loss MSE: 9.234675407409668, Loss KL: 0.12031802535057068\n",
      "Epoch [120/1000], Loss MSE: 10.750088691711426, Loss KL: 0.12005878239870071\n",
      "Epoch [121/1000], Loss MSE: 6.735654354095459, Loss KL: 0.11983028054237366\n",
      "Epoch [122/1000], Loss MSE: 8.297758102416992, Loss KL: 0.11957360804080963\n",
      "Epoch [123/1000], Loss MSE: 8.728120803833008, Loss KL: 0.11931449174880981\n",
      "Epoch [124/1000], Loss MSE: 9.44784927368164, Loss KL: 0.1190788745880127\n",
      "Epoch [125/1000], Loss MSE: 9.617380142211914, Loss KL: 0.11882981657981873\n",
      "Epoch [126/1000], Loss MSE: 6.6902947425842285, Loss KL: 0.11857201904058456\n",
      "Epoch [127/1000], Loss MSE: 8.332258224487305, Loss KL: 0.118350550532341\n",
      "Epoch [128/1000], Loss MSE: 9.846988677978516, Loss KL: 0.11810634285211563\n",
      "Epoch [129/1000], Loss MSE: 8.819690704345703, Loss KL: 0.11787889152765274\n",
      "Epoch [130/1000], Loss MSE: 9.460476875305176, Loss KL: 0.11766338348388672\n",
      "Epoch [131/1000], Loss MSE: 5.46211576461792, Loss KL: 0.11745093762874603\n",
      "Epoch [132/1000], Loss MSE: 7.6556830406188965, Loss KL: 0.11723142117261887\n",
      "Epoch [133/1000], Loss MSE: 8.138708114624023, Loss KL: 0.11701605468988419\n",
      "Epoch [134/1000], Loss MSE: 8.48991870880127, Loss KL: 0.11677397787570953\n",
      "Epoch [135/1000], Loss MSE: 7.858217716217041, Loss KL: 0.11654798686504364\n",
      "Epoch [136/1000], Loss MSE: 6.870453834533691, Loss KL: 0.1163407415151596\n",
      "Epoch [137/1000], Loss MSE: 8.814837455749512, Loss KL: 0.11614704132080078\n",
      "Epoch [138/1000], Loss MSE: 8.889481544494629, Loss KL: 0.11596041172742844\n",
      "Epoch [139/1000], Loss MSE: 8.740729331970215, Loss KL: 0.1157592162489891\n",
      "Epoch [140/1000], Loss MSE: 7.208055019378662, Loss KL: 0.11556102335453033\n",
      "Epoch [141/1000], Loss MSE: 9.944939613342285, Loss KL: 0.11535117775201797\n",
      "Epoch [142/1000], Loss MSE: 6.156369209289551, Loss KL: 0.11515898257493973\n",
      "Epoch [143/1000], Loss MSE: 8.473867416381836, Loss KL: 0.11496271938085556\n",
      "Epoch [144/1000], Loss MSE: 8.006033897399902, Loss KL: 0.1147407814860344\n",
      "Epoch [145/1000], Loss MSE: 7.581212043762207, Loss KL: 0.11456094682216644\n",
      "Epoch [146/1000], Loss MSE: 11.312801361083984, Loss KL: 0.1143491119146347\n",
      "Epoch [147/1000], Loss MSE: 9.457967758178711, Loss KL: 0.11415950208902359\n",
      "Epoch [148/1000], Loss MSE: 8.410665512084961, Loss KL: 0.1139802560210228\n",
      "Epoch [149/1000], Loss MSE: 7.614515781402588, Loss KL: 0.11379379034042358\n",
      "Epoch [150/1000], Loss MSE: 10.399513244628906, Loss KL: 0.1135869100689888\n",
      "Epoch [151/1000], Loss MSE: 7.491436958312988, Loss KL: 0.1134282648563385\n",
      "Epoch [152/1000], Loss MSE: 6.855429649353027, Loss KL: 0.11325303465127945\n",
      "Epoch [153/1000], Loss MSE: 8.497945785522461, Loss KL: 0.11305803805589676\n",
      "Epoch [154/1000], Loss MSE: 5.022213935852051, Loss KL: 0.1128678172826767\n",
      "Epoch [155/1000], Loss MSE: 8.418778419494629, Loss KL: 0.11269176006317139\n",
      "Epoch [156/1000], Loss MSE: 6.776898384094238, Loss KL: 0.112533338367939\n",
      "Epoch [157/1000], Loss MSE: 6.325680732727051, Loss KL: 0.11235899478197098\n",
      "Epoch [158/1000], Loss MSE: 4.967838287353516, Loss KL: 0.11218515038490295\n",
      "Epoch [159/1000], Loss MSE: 8.754176139831543, Loss KL: 0.11199619621038437\n",
      "Epoch [160/1000], Loss MSE: 7.529611110687256, Loss KL: 0.1118592619895935\n",
      "Epoch [161/1000], Loss MSE: 6.7708306312561035, Loss KL: 0.1117001622915268\n",
      "Epoch [162/1000], Loss MSE: 12.171195030212402, Loss KL: 0.11152926832437515\n",
      "Epoch [163/1000], Loss MSE: 4.4120259284973145, Loss KL: 0.11137716472148895\n",
      "Epoch [164/1000], Loss MSE: 9.331335067749023, Loss KL: 0.11122097074985504\n",
      "Epoch [165/1000], Loss MSE: 7.235933780670166, Loss KL: 0.11108081787824631\n",
      "Epoch [166/1000], Loss MSE: 6.125265121459961, Loss KL: 0.11097978800535202\n",
      "Epoch [167/1000], Loss MSE: 4.787532329559326, Loss KL: 0.11083626002073288\n",
      "Epoch [168/1000], Loss MSE: 8.073552131652832, Loss KL: 0.11069174110889435\n",
      "Epoch [169/1000], Loss MSE: 6.6751389503479, Loss KL: 0.1105479896068573\n",
      "Epoch [170/1000], Loss MSE: 7.68365478515625, Loss KL: 0.11044316738843918\n",
      "Epoch [171/1000], Loss MSE: 8.750558853149414, Loss KL: 0.11031665652990341\n",
      "Epoch [172/1000], Loss MSE: 8.970056533813477, Loss KL: 0.11017732322216034\n",
      "Epoch [173/1000], Loss MSE: 5.290200233459473, Loss KL: 0.11006256192922592\n",
      "Epoch [174/1000], Loss MSE: 8.531900405883789, Loss KL: 0.10994314402341843\n",
      "Epoch [175/1000], Loss MSE: 5.733692646026611, Loss KL: 0.10981713980436325\n",
      "Epoch [176/1000], Loss MSE: 8.791542053222656, Loss KL: 0.10973258316516876\n",
      "Epoch [177/1000], Loss MSE: 7.546736717224121, Loss KL: 0.10963591188192368\n",
      "Epoch [178/1000], Loss MSE: 8.764001846313477, Loss KL: 0.10954626649618149\n",
      "Epoch [179/1000], Loss MSE: 9.982474327087402, Loss KL: 0.10944826900959015\n",
      "Epoch [180/1000], Loss MSE: 12.004447937011719, Loss KL: 0.10935832560062408\n",
      "Epoch [181/1000], Loss MSE: 7.366654872894287, Loss KL: 0.10924600809812546\n",
      "Epoch [182/1000], Loss MSE: 8.928668022155762, Loss KL: 0.10918312519788742\n",
      "Epoch [183/1000], Loss MSE: 5.928634166717529, Loss KL: 0.1091485396027565\n",
      "Epoch [184/1000], Loss MSE: 4.824376106262207, Loss KL: 0.10910695791244507\n",
      "Epoch [185/1000], Loss MSE: 5.8319315910339355, Loss KL: 0.10902140289545059\n",
      "Epoch [186/1000], Loss MSE: 8.109289169311523, Loss KL: 0.10893204808235168\n",
      "Epoch [187/1000], Loss MSE: 9.54608154296875, Loss KL: 0.10890328139066696\n",
      "Epoch [188/1000], Loss MSE: 8.285665512084961, Loss KL: 0.10887574404478073\n",
      "Epoch [189/1000], Loss MSE: 6.914340972900391, Loss KL: 0.10882069170475006\n",
      "Epoch [190/1000], Loss MSE: 7.40509557723999, Loss KL: 0.10878998786211014\n",
      "Epoch [191/1000], Loss MSE: 8.344812393188477, Loss KL: 0.10875459760427475\n",
      "Epoch [192/1000], Loss MSE: 9.229901313781738, Loss KL: 0.10875598341226578\n",
      "Epoch [193/1000], Loss MSE: 9.761024475097656, Loss KL: 0.10872893780469894\n",
      "Epoch [194/1000], Loss MSE: 7.019297122955322, Loss KL: 0.10874933004379272\n",
      "Epoch [195/1000], Loss MSE: 5.079346656799316, Loss KL: 0.10872948914766312\n",
      "Epoch [196/1000], Loss MSE: 6.205957412719727, Loss KL: 0.10871385782957077\n",
      "Epoch [197/1000], Loss MSE: 8.268219947814941, Loss KL: 0.10870958119630814\n",
      "Epoch [198/1000], Loss MSE: 6.38491678237915, Loss KL: 0.10874950885772705\n",
      "Epoch [199/1000], Loss MSE: 6.725789546966553, Loss KL: 0.10878012329339981\n",
      "Epoch [200/1000], Loss MSE: 8.951897621154785, Loss KL: 0.10880398005247116\n",
      "Epoch [201/1000], Loss MSE: 8.103789329528809, Loss KL: 0.10882941633462906\n",
      "Epoch [202/1000], Loss MSE: 5.304728984832764, Loss KL: 0.10883482545614243\n",
      "Epoch [203/1000], Loss MSE: 7.871541500091553, Loss KL: 0.10886109620332718\n",
      "Epoch [204/1000], Loss MSE: 8.82951545715332, Loss KL: 0.10892180353403091\n",
      "Epoch [205/1000], Loss MSE: 6.697812557220459, Loss KL: 0.10897522419691086\n",
      "Epoch [206/1000], Loss MSE: 7.3748345375061035, Loss KL: 0.10902965813875198\n",
      "Epoch [207/1000], Loss MSE: 7.9523186683654785, Loss KL: 0.10908783972263336\n",
      "Epoch [208/1000], Loss MSE: 5.856949806213379, Loss KL: 0.10914883762598038\n",
      "Epoch [209/1000], Loss MSE: 7.53476619720459, Loss KL: 0.10919573903083801\n",
      "Epoch [210/1000], Loss MSE: 5.697527885437012, Loss KL: 0.10930418968200684\n",
      "Epoch [211/1000], Loss MSE: 8.276700019836426, Loss KL: 0.10942388325929642\n",
      "Epoch [212/1000], Loss MSE: 9.49696159362793, Loss KL: 0.10951625555753708\n",
      "Epoch [213/1000], Loss MSE: 7.015158653259277, Loss KL: 0.10962510854005814\n",
      "Epoch [214/1000], Loss MSE: 12.085060119628906, Loss KL: 0.10975091904401779\n",
      "Epoch [215/1000], Loss MSE: 10.840555191040039, Loss KL: 0.10986104607582092\n",
      "Epoch [216/1000], Loss MSE: 7.506880283355713, Loss KL: 0.10997254401445389\n",
      "Epoch [217/1000], Loss MSE: 7.785298824310303, Loss KL: 0.1100989505648613\n",
      "Epoch [218/1000], Loss MSE: 8.001374244689941, Loss KL: 0.11022548377513885\n",
      "Epoch [219/1000], Loss MSE: 6.112624168395996, Loss KL: 0.11033458262681961\n",
      "Epoch [220/1000], Loss MSE: 10.23994255065918, Loss KL: 0.11044584959745407\n",
      "Epoch [221/1000], Loss MSE: 7.857058525085449, Loss KL: 0.11055907607078552\n",
      "Epoch [222/1000], Loss MSE: 7.896073818206787, Loss KL: 0.1106891930103302\n",
      "Epoch [223/1000], Loss MSE: 9.46382999420166, Loss KL: 0.11082751303911209\n",
      "Epoch [224/1000], Loss MSE: 9.421497344970703, Loss KL: 0.11099224537611008\n",
      "Epoch [225/1000], Loss MSE: 8.223241806030273, Loss KL: 0.11113162338733673\n",
      "Epoch [226/1000], Loss MSE: 8.192361831665039, Loss KL: 0.11124034970998764\n",
      "Epoch [227/1000], Loss MSE: 5.715939521789551, Loss KL: 0.11138610541820526\n",
      "Epoch [228/1000], Loss MSE: 5.671652317047119, Loss KL: 0.11154574900865555\n",
      "Epoch [229/1000], Loss MSE: 5.834028720855713, Loss KL: 0.11170156300067902\n",
      "Epoch [230/1000], Loss MSE: 9.395563125610352, Loss KL: 0.11184937506914139\n",
      "Epoch [231/1000], Loss MSE: 8.101740837097168, Loss KL: 0.11203934997320175\n",
      "Epoch [232/1000], Loss MSE: 10.247197151184082, Loss KL: 0.11222482472658157\n",
      "Epoch [233/1000], Loss MSE: 7.0898613929748535, Loss KL: 0.1124243289232254\n",
      "Epoch [234/1000], Loss MSE: 6.223669528961182, Loss KL: 0.11261598020792007\n",
      "Epoch [235/1000], Loss MSE: 8.304896354675293, Loss KL: 0.11280640214681625\n",
      "Epoch [236/1000], Loss MSE: 6.415410041809082, Loss KL: 0.11300832778215408\n",
      "Epoch [237/1000], Loss MSE: 7.024965763092041, Loss KL: 0.11321422457695007\n",
      "Epoch [238/1000], Loss MSE: 5.537554740905762, Loss KL: 0.11342129856348038\n",
      "Epoch [239/1000], Loss MSE: 9.06091022491455, Loss KL: 0.1136334240436554\n",
      "Epoch [240/1000], Loss MSE: 10.370827674865723, Loss KL: 0.1138245165348053\n",
      "Epoch [241/1000], Loss MSE: 7.969256401062012, Loss KL: 0.11403920501470566\n",
      "Epoch [242/1000], Loss MSE: 8.043778419494629, Loss KL: 0.11424411833286285\n",
      "Epoch [243/1000], Loss MSE: 6.182157516479492, Loss KL: 0.1144474446773529\n",
      "Epoch [244/1000], Loss MSE: 9.559860229492188, Loss KL: 0.11464841663837433\n",
      "Epoch [245/1000], Loss MSE: 10.43961238861084, Loss KL: 0.11484857648611069\n",
      "Epoch [246/1000], Loss MSE: 7.773167610168457, Loss KL: 0.11509063839912415\n",
      "Epoch [247/1000], Loss MSE: 6.648795127868652, Loss KL: 0.1153198778629303\n",
      "Epoch [248/1000], Loss MSE: 7.976053714752197, Loss KL: 0.11553005874156952\n",
      "Epoch [249/1000], Loss MSE: 7.981010437011719, Loss KL: 0.11577875912189484\n",
      "Epoch [250/1000], Loss MSE: 7.533088684082031, Loss KL: 0.11604997515678406\n",
      "Epoch [251/1000], Loss MSE: 8.045220375061035, Loss KL: 0.11631661653518677\n",
      "Epoch [252/1000], Loss MSE: 5.681719779968262, Loss KL: 0.11656280606985092\n",
      "Epoch [253/1000], Loss MSE: 9.743932723999023, Loss KL: 0.1168111115694046\n",
      "Epoch [254/1000], Loss MSE: 6.863226890563965, Loss KL: 0.11707104742527008\n",
      "Epoch [255/1000], Loss MSE: 9.334247589111328, Loss KL: 0.1173124685883522\n",
      "Epoch [256/1000], Loss MSE: 6.795515537261963, Loss KL: 0.11755625158548355\n",
      "Epoch [257/1000], Loss MSE: 5.639352321624756, Loss KL: 0.11780892312526703\n",
      "Epoch [258/1000], Loss MSE: 6.52730655670166, Loss KL: 0.11805544048547745\n",
      "Epoch [259/1000], Loss MSE: 6.387423515319824, Loss KL: 0.11832934617996216\n",
      "Epoch [260/1000], Loss MSE: 8.368731498718262, Loss KL: 0.11858729273080826\n",
      "Epoch [261/1000], Loss MSE: 7.7716779708862305, Loss KL: 0.11883611232042313\n",
      "Epoch [262/1000], Loss MSE: 7.565152645111084, Loss KL: 0.11911911517381668\n",
      "Epoch [263/1000], Loss MSE: 5.787298202514648, Loss KL: 0.11938493698835373\n",
      "Epoch [264/1000], Loss MSE: 4.898370742797852, Loss KL: 0.11966865509748459\n",
      "Epoch [265/1000], Loss MSE: 7.050455570220947, Loss KL: 0.11994006484746933\n",
      "Epoch [266/1000], Loss MSE: 6.648739814758301, Loss KL: 0.12022227793931961\n",
      "Epoch [267/1000], Loss MSE: 9.083096504211426, Loss KL: 0.12050680816173553\n",
      "Epoch [268/1000], Loss MSE: 8.061195373535156, Loss KL: 0.12078412622213364\n",
      "Epoch [269/1000], Loss MSE: 8.864059448242188, Loss KL: 0.12106629461050034\n",
      "Epoch [270/1000], Loss MSE: 5.520012855529785, Loss KL: 0.1213354542851448\n",
      "Epoch [271/1000], Loss MSE: 11.182270050048828, Loss KL: 0.1216135174036026\n",
      "Epoch [272/1000], Loss MSE: 6.680410861968994, Loss KL: 0.12192726135253906\n",
      "Epoch [273/1000], Loss MSE: 7.0805253982543945, Loss KL: 0.12221448123455048\n",
      "Epoch [274/1000], Loss MSE: 7.467194557189941, Loss KL: 0.12250896543264389\n",
      "Epoch [275/1000], Loss MSE: 6.9294939041137695, Loss KL: 0.12280435860157013\n",
      "Epoch [276/1000], Loss MSE: 4.021054744720459, Loss KL: 0.12309012562036514\n",
      "Epoch [277/1000], Loss MSE: 8.677538871765137, Loss KL: 0.12340594828128815\n",
      "Epoch [278/1000], Loss MSE: 5.582655906677246, Loss KL: 0.12369833886623383\n",
      "Epoch [279/1000], Loss MSE: 8.656332969665527, Loss KL: 0.12399540841579437\n",
      "Epoch [280/1000], Loss MSE: 5.125422477722168, Loss KL: 0.12429497390985489\n",
      "Epoch [281/1000], Loss MSE: 9.187067031860352, Loss KL: 0.12458153814077377\n",
      "Epoch [282/1000], Loss MSE: 5.779038429260254, Loss KL: 0.12489856034517288\n",
      "Epoch [283/1000], Loss MSE: 5.831797122955322, Loss KL: 0.12519656121730804\n",
      "Epoch [284/1000], Loss MSE: 9.613248825073242, Loss KL: 0.12551842629909515\n",
      "Epoch [285/1000], Loss MSE: 7.958466529846191, Loss KL: 0.12582248449325562\n",
      "Epoch [286/1000], Loss MSE: 8.515833854675293, Loss KL: 0.1261269897222519\n",
      "Epoch [287/1000], Loss MSE: 5.6907758712768555, Loss KL: 0.1264427900314331\n",
      "Epoch [288/1000], Loss MSE: 4.723013401031494, Loss KL: 0.12673529982566833\n",
      "Epoch [289/1000], Loss MSE: 7.547303676605225, Loss KL: 0.12705017626285553\n",
      "Epoch [290/1000], Loss MSE: 7.948159217834473, Loss KL: 0.12734924256801605\n",
      "Epoch [291/1000], Loss MSE: 6.792788505554199, Loss KL: 0.1276627779006958\n",
      "Epoch [292/1000], Loss MSE: 4.814828395843506, Loss KL: 0.1279733031988144\n",
      "Epoch [293/1000], Loss MSE: 8.408127784729004, Loss KL: 0.1282978653907776\n",
      "Epoch [294/1000], Loss MSE: 6.864524841308594, Loss KL: 0.1286211907863617\n",
      "Epoch [295/1000], Loss MSE: 5.888588905334473, Loss KL: 0.12894368171691895\n",
      "Epoch [296/1000], Loss MSE: 7.604917049407959, Loss KL: 0.12925775349140167\n",
      "Epoch [297/1000], Loss MSE: 7.3138427734375, Loss KL: 0.12959718704223633\n",
      "Epoch [298/1000], Loss MSE: 5.212374210357666, Loss KL: 0.12993405759334564\n",
      "Epoch [299/1000], Loss MSE: 7.98868465423584, Loss KL: 0.13024865090847015\n",
      "Epoch [300/1000], Loss MSE: 4.937465190887451, Loss KL: 0.13057157397270203\n",
      "Epoch [301/1000], Loss MSE: 6.851070404052734, Loss KL: 0.13088209927082062\n",
      "Epoch [302/1000], Loss MSE: 6.360085487365723, Loss KL: 0.1312105655670166\n",
      "Epoch [303/1000], Loss MSE: 10.081064224243164, Loss KL: 0.1315528303384781\n",
      "Epoch [304/1000], Loss MSE: 5.343176364898682, Loss KL: 0.13189645111560822\n",
      "Epoch [305/1000], Loss MSE: 5.898201942443848, Loss KL: 0.13221362233161926\n",
      "Epoch [306/1000], Loss MSE: 6.23018217086792, Loss KL: 0.1325511485338211\n",
      "Epoch [307/1000], Loss MSE: 6.256644248962402, Loss KL: 0.13289934396743774\n",
      "Epoch [308/1000], Loss MSE: 6.563370704650879, Loss KL: 0.13323208689689636\n",
      "Epoch [309/1000], Loss MSE: 7.604933738708496, Loss KL: 0.1336057484149933\n",
      "Epoch [310/1000], Loss MSE: 4.8561506271362305, Loss KL: 0.13394704461097717\n",
      "Epoch [311/1000], Loss MSE: 3.971865177154541, Loss KL: 0.13430610299110413\n",
      "Epoch [312/1000], Loss MSE: 6.3838396072387695, Loss KL: 0.1346481442451477\n",
      "Epoch [313/1000], Loss MSE: 4.591427803039551, Loss KL: 0.13500705361366272\n",
      "Epoch [314/1000], Loss MSE: 8.051630020141602, Loss KL: 0.13534139096736908\n",
      "Epoch [315/1000], Loss MSE: 5.967101097106934, Loss KL: 0.13570593297481537\n",
      "Epoch [316/1000], Loss MSE: 5.473938941955566, Loss KL: 0.1360648274421692\n",
      "Epoch [317/1000], Loss MSE: 6.0916924476623535, Loss KL: 0.1364395022392273\n",
      "Epoch [318/1000], Loss MSE: 4.234230995178223, Loss KL: 0.13680212199687958\n",
      "Epoch [319/1000], Loss MSE: 6.3496599197387695, Loss KL: 0.13714487850666046\n",
      "Epoch [320/1000], Loss MSE: 5.474461555480957, Loss KL: 0.13751204311847687\n",
      "Epoch [321/1000], Loss MSE: 4.513815402984619, Loss KL: 0.13786381483078003\n",
      "Epoch [322/1000], Loss MSE: 6.521708011627197, Loss KL: 0.1382163017988205\n",
      "Epoch [323/1000], Loss MSE: 6.6856513023376465, Loss KL: 0.1385875791311264\n",
      "Epoch [324/1000], Loss MSE: 5.967380046844482, Loss KL: 0.13897287845611572\n",
      "Epoch [325/1000], Loss MSE: 7.005986213684082, Loss KL: 0.13936159014701843\n",
      "Epoch [326/1000], Loss MSE: 4.970438003540039, Loss KL: 0.13975577056407928\n",
      "Epoch [327/1000], Loss MSE: 5.543200492858887, Loss KL: 0.14013269543647766\n",
      "Epoch [328/1000], Loss MSE: 4.908604621887207, Loss KL: 0.14049704372882843\n",
      "Epoch [329/1000], Loss MSE: 6.946403503417969, Loss KL: 0.14086006581783295\n",
      "Epoch [330/1000], Loss MSE: 7.111546516418457, Loss KL: 0.14123082160949707\n",
      "Epoch [331/1000], Loss MSE: 4.891149044036865, Loss KL: 0.14158445596694946\n",
      "Epoch [332/1000], Loss MSE: 6.7438507080078125, Loss KL: 0.14196975529193878\n",
      "Epoch [333/1000], Loss MSE: 5.575507640838623, Loss KL: 0.142353817820549\n",
      "Epoch [334/1000], Loss MSE: 8.112199783325195, Loss KL: 0.1427418291568756\n",
      "Epoch [335/1000], Loss MSE: 5.508984565734863, Loss KL: 0.14314734935760498\n",
      "Epoch [336/1000], Loss MSE: 6.260293483734131, Loss KL: 0.14356130361557007\n",
      "Epoch [337/1000], Loss MSE: 6.52059268951416, Loss KL: 0.14396019279956818\n",
      "Epoch [338/1000], Loss MSE: 4.246967315673828, Loss KL: 0.14436791837215424\n",
      "Epoch [339/1000], Loss MSE: 5.9549455642700195, Loss KL: 0.1447610706090927\n",
      "Epoch [340/1000], Loss MSE: 5.23878288269043, Loss KL: 0.14516089856624603\n",
      "Epoch [341/1000], Loss MSE: 5.390754222869873, Loss KL: 0.14558745920658112\n",
      "Epoch [342/1000], Loss MSE: 4.196284294128418, Loss KL: 0.14599792659282684\n",
      "Epoch [343/1000], Loss MSE: 6.094385147094727, Loss KL: 0.14641977846622467\n",
      "Epoch [344/1000], Loss MSE: 6.4060235023498535, Loss KL: 0.146833136677742\n",
      "Epoch [345/1000], Loss MSE: 5.147889614105225, Loss KL: 0.1472758799791336\n",
      "Epoch [346/1000], Loss MSE: 3.145153045654297, Loss KL: 0.14771617949008942\n",
      "Epoch [347/1000], Loss MSE: 4.6903395652771, Loss KL: 0.14815765619277954\n",
      "Epoch [348/1000], Loss MSE: 5.518739223480225, Loss KL: 0.14859610795974731\n",
      "Epoch [349/1000], Loss MSE: 4.8528971672058105, Loss KL: 0.1490396410226822\n",
      "Epoch [350/1000], Loss MSE: 4.771862983703613, Loss KL: 0.14946389198303223\n",
      "Epoch [351/1000], Loss MSE: 5.456246376037598, Loss KL: 0.14990130066871643\n",
      "Epoch [352/1000], Loss MSE: 4.496420860290527, Loss KL: 0.1503271907567978\n",
      "Epoch [353/1000], Loss MSE: 4.335799217224121, Loss KL: 0.15076060593128204\n",
      "Epoch [354/1000], Loss MSE: 5.243386745452881, Loss KL: 0.151180699467659\n",
      "Epoch [355/1000], Loss MSE: 4.8375139236450195, Loss KL: 0.1516103446483612\n",
      "Epoch [356/1000], Loss MSE: 7.106229305267334, Loss KL: 0.15202321112155914\n",
      "Epoch [357/1000], Loss MSE: 6.405519962310791, Loss KL: 0.1524478644132614\n",
      "Epoch [358/1000], Loss MSE: 4.257319450378418, Loss KL: 0.15292364358901978\n",
      "Epoch [359/1000], Loss MSE: 3.850935697555542, Loss KL: 0.15336082875728607\n",
      "Epoch [360/1000], Loss MSE: 4.274447441101074, Loss KL: 0.15381160378456116\n",
      "Epoch [361/1000], Loss MSE: 6.169795989990234, Loss KL: 0.1542399823665619\n",
      "Epoch [362/1000], Loss MSE: 6.385487079620361, Loss KL: 0.15467208623886108\n",
      "Epoch [363/1000], Loss MSE: 4.409708023071289, Loss KL: 0.15509307384490967\n",
      "Epoch [364/1000], Loss MSE: 3.56980562210083, Loss KL: 0.15555036067962646\n",
      "Epoch [365/1000], Loss MSE: 6.841801643371582, Loss KL: 0.15600182116031647\n",
      "Epoch [366/1000], Loss MSE: 5.878872871398926, Loss KL: 0.1564236581325531\n",
      "Epoch [367/1000], Loss MSE: 5.383246421813965, Loss KL: 0.15687187016010284\n",
      "Epoch [368/1000], Loss MSE: 5.029096603393555, Loss KL: 0.15731747448444366\n",
      "Epoch [369/1000], Loss MSE: 3.4520232677459717, Loss KL: 0.15778231620788574\n",
      "Epoch [370/1000], Loss MSE: 3.4099223613739014, Loss KL: 0.15822236239910126\n",
      "Epoch [371/1000], Loss MSE: 3.1053824424743652, Loss KL: 0.158668652176857\n",
      "Epoch [372/1000], Loss MSE: 4.709949970245361, Loss KL: 0.1591155081987381\n",
      "Epoch [373/1000], Loss MSE: 4.5617499351501465, Loss KL: 0.15957312285900116\n",
      "Epoch [374/1000], Loss MSE: 4.399901390075684, Loss KL: 0.16002283990383148\n",
      "Epoch [375/1000], Loss MSE: 5.374906539916992, Loss KL: 0.16046835482120514\n",
      "Epoch [376/1000], Loss MSE: 3.4170613288879395, Loss KL: 0.16091877222061157\n",
      "Epoch [377/1000], Loss MSE: 2.86594820022583, Loss KL: 0.16139622032642365\n",
      "Epoch [378/1000], Loss MSE: 3.2823193073272705, Loss KL: 0.16185244917869568\n",
      "Epoch [379/1000], Loss MSE: 4.054001808166504, Loss KL: 0.16230277717113495\n",
      "Epoch [380/1000], Loss MSE: 6.37362813949585, Loss KL: 0.16276130080223083\n",
      "Epoch [381/1000], Loss MSE: 5.38818359375, Loss KL: 0.16324801743030548\n",
      "Epoch [382/1000], Loss MSE: 5.0443878173828125, Loss KL: 0.16373701393604279\n",
      "Epoch [383/1000], Loss MSE: 3.0362682342529297, Loss KL: 0.16420497000217438\n",
      "Epoch [384/1000], Loss MSE: 5.443187713623047, Loss KL: 0.16466915607452393\n",
      "Epoch [385/1000], Loss MSE: 4.5147294998168945, Loss KL: 0.16515544056892395\n",
      "Epoch [386/1000], Loss MSE: 3.3633766174316406, Loss KL: 0.1656482219696045\n",
      "Epoch [387/1000], Loss MSE: 3.3514602184295654, Loss KL: 0.16613979637622833\n",
      "Epoch [388/1000], Loss MSE: 3.501014232635498, Loss KL: 0.16663150489330292\n",
      "Epoch [389/1000], Loss MSE: 3.8671817779541016, Loss KL: 0.1671026200056076\n",
      "Epoch [390/1000], Loss MSE: 3.529750347137451, Loss KL: 0.16758401691913605\n",
      "Epoch [391/1000], Loss MSE: 4.205954551696777, Loss KL: 0.16808302700519562\n",
      "Epoch [392/1000], Loss MSE: 5.35313606262207, Loss KL: 0.16856452822685242\n",
      "Epoch [393/1000], Loss MSE: 5.157646179199219, Loss KL: 0.16905201971530914\n",
      "Epoch [394/1000], Loss MSE: 5.373343467712402, Loss KL: 0.16954652965068817\n",
      "Epoch [395/1000], Loss MSE: 4.269432544708252, Loss KL: 0.17005513608455658\n",
      "Epoch [396/1000], Loss MSE: 4.096228122711182, Loss KL: 0.17052653431892395\n",
      "Epoch [397/1000], Loss MSE: 2.84602427482605, Loss KL: 0.1710023432970047\n",
      "Epoch [398/1000], Loss MSE: 4.283854007720947, Loss KL: 0.17147324979305267\n",
      "Epoch [399/1000], Loss MSE: 2.9329960346221924, Loss KL: 0.17195530235767365\n",
      "Epoch [400/1000], Loss MSE: 4.863311767578125, Loss KL: 0.1724652647972107\n",
      "Epoch [401/1000], Loss MSE: 4.982419967651367, Loss KL: 0.17295697331428528\n",
      "Epoch [402/1000], Loss MSE: 3.02364182472229, Loss KL: 0.17343787848949432\n",
      "Epoch [403/1000], Loss MSE: 4.372977256774902, Loss KL: 0.17393137514591217\n",
      "Epoch [404/1000], Loss MSE: 4.54685640335083, Loss KL: 0.1744288057088852\n",
      "Epoch [405/1000], Loss MSE: 3.3371613025665283, Loss KL: 0.17491549253463745\n",
      "Epoch [406/1000], Loss MSE: 3.5828239917755127, Loss KL: 0.17541800439357758\n",
      "Epoch [407/1000], Loss MSE: 3.4684455394744873, Loss KL: 0.1759207397699356\n",
      "Epoch [408/1000], Loss MSE: 2.860673189163208, Loss KL: 0.1764131784439087\n",
      "Epoch [409/1000], Loss MSE: 4.171477317810059, Loss KL: 0.1768970936536789\n",
      "Epoch [410/1000], Loss MSE: 3.930973768234253, Loss KL: 0.17742906510829926\n",
      "Epoch [411/1000], Loss MSE: 4.801400184631348, Loss KL: 0.17793889343738556\n",
      "Epoch [412/1000], Loss MSE: 2.879957914352417, Loss KL: 0.17843981087207794\n",
      "Epoch [413/1000], Loss MSE: 3.916304349899292, Loss KL: 0.17894890904426575\n",
      "Epoch [414/1000], Loss MSE: 3.6057369709014893, Loss KL: 0.17942185699939728\n",
      "Epoch [415/1000], Loss MSE: 4.732550621032715, Loss KL: 0.1798904538154602\n",
      "Epoch [416/1000], Loss MSE: 3.657991886138916, Loss KL: 0.18039356172084808\n",
      "Epoch [417/1000], Loss MSE: 4.779911518096924, Loss KL: 0.18092051148414612\n",
      "Epoch [418/1000], Loss MSE: 4.556028366088867, Loss KL: 0.18141326308250427\n",
      "Epoch [419/1000], Loss MSE: 3.3912479877471924, Loss KL: 0.18192702531814575\n",
      "Epoch [420/1000], Loss MSE: 3.490734100341797, Loss KL: 0.18243055045604706\n",
      "Epoch [421/1000], Loss MSE: 3.347238063812256, Loss KL: 0.18294686079025269\n",
      "Epoch [422/1000], Loss MSE: 2.9135794639587402, Loss KL: 0.18344244360923767\n",
      "Epoch [423/1000], Loss MSE: 4.7424492835998535, Loss KL: 0.18393656611442566\n",
      "Epoch [424/1000], Loss MSE: 3.8747551441192627, Loss KL: 0.18444785475730896\n",
      "Epoch [425/1000], Loss MSE: 4.287611961364746, Loss KL: 0.18497124314308167\n",
      "Epoch [426/1000], Loss MSE: 3.41255521774292, Loss KL: 0.185485377907753\n",
      "Epoch [427/1000], Loss MSE: 3.4740188121795654, Loss KL: 0.1860056072473526\n",
      "Epoch [428/1000], Loss MSE: 2.890169143676758, Loss KL: 0.18652145564556122\n",
      "Epoch [429/1000], Loss MSE: 3.308903455734253, Loss KL: 0.18705236911773682\n",
      "Epoch [430/1000], Loss MSE: 2.965416431427002, Loss KL: 0.18755948543548584\n",
      "Epoch [431/1000], Loss MSE: 3.726606845855713, Loss KL: 0.1880611926317215\n",
      "Epoch [432/1000], Loss MSE: 3.5212130546569824, Loss KL: 0.1885959953069687\n",
      "Epoch [433/1000], Loss MSE: 2.7964000701904297, Loss KL: 0.1891142576932907\n",
      "Epoch [434/1000], Loss MSE: 3.4658076763153076, Loss KL: 0.18965111672878265\n",
      "Epoch [435/1000], Loss MSE: 4.035266399383545, Loss KL: 0.1902214139699936\n",
      "Epoch [436/1000], Loss MSE: 2.459932327270508, Loss KL: 0.19076427817344666\n",
      "Epoch [437/1000], Loss MSE: 3.4971041679382324, Loss KL: 0.19127875566482544\n",
      "Epoch [438/1000], Loss MSE: 2.7361412048339844, Loss KL: 0.19181020557880402\n",
      "Epoch [439/1000], Loss MSE: 2.2111003398895264, Loss KL: 0.19233252108097076\n",
      "Epoch [440/1000], Loss MSE: 3.395521879196167, Loss KL: 0.1928595006465912\n",
      "Epoch [441/1000], Loss MSE: 3.412782669067383, Loss KL: 0.19342389702796936\n",
      "Epoch [442/1000], Loss MSE: 3.563446044921875, Loss KL: 0.1939752697944641\n",
      "Epoch [443/1000], Loss MSE: 4.031155109405518, Loss KL: 0.194504976272583\n",
      "Epoch [444/1000], Loss MSE: 3.782966136932373, Loss KL: 0.1950494647026062\n",
      "Epoch [445/1000], Loss MSE: 3.5996410846710205, Loss KL: 0.19560156762599945\n",
      "Epoch [446/1000], Loss MSE: 2.3845009803771973, Loss KL: 0.19615957140922546\n",
      "Epoch [447/1000], Loss MSE: 3.9693031311035156, Loss KL: 0.19671732187271118\n",
      "Epoch [448/1000], Loss MSE: 2.7460758686065674, Loss KL: 0.1972597986459732\n",
      "Epoch [449/1000], Loss MSE: 2.768843173980713, Loss KL: 0.19778625667095184\n",
      "Epoch [450/1000], Loss MSE: 2.1714072227478027, Loss KL: 0.1983182430267334\n",
      "Epoch [451/1000], Loss MSE: 2.569070816040039, Loss KL: 0.19886407256126404\n",
      "Epoch [452/1000], Loss MSE: 3.1715080738067627, Loss KL: 0.19939562678337097\n",
      "Epoch [453/1000], Loss MSE: 1.906863808631897, Loss KL: 0.19992130994796753\n",
      "Epoch [454/1000], Loss MSE: 3.8445773124694824, Loss KL: 0.2004498690366745\n",
      "Epoch [455/1000], Loss MSE: 2.1680045127868652, Loss KL: 0.2009986937046051\n",
      "Epoch [456/1000], Loss MSE: 3.623109817504883, Loss KL: 0.201554074883461\n",
      "Epoch [457/1000], Loss MSE: 2.623464345932007, Loss KL: 0.20210735499858856\n",
      "Epoch [458/1000], Loss MSE: 3.1034598350524902, Loss KL: 0.20266349613666534\n",
      "Epoch [459/1000], Loss MSE: 5.0557146072387695, Loss KL: 0.20323343575000763\n",
      "Epoch [460/1000], Loss MSE: 3.4325153827667236, Loss KL: 0.20380236208438873\n",
      "Epoch [461/1000], Loss MSE: 3.136840343475342, Loss KL: 0.20435737073421478\n",
      "Epoch [462/1000], Loss MSE: 3.049370288848877, Loss KL: 0.20491977035999298\n",
      "Epoch [463/1000], Loss MSE: 1.9560298919677734, Loss KL: 0.2054450511932373\n",
      "Epoch [464/1000], Loss MSE: 2.164140224456787, Loss KL: 0.20598995685577393\n",
      "Epoch [465/1000], Loss MSE: 3.4387059211730957, Loss KL: 0.20651811361312866\n",
      "Epoch [466/1000], Loss MSE: 3.2594571113586426, Loss KL: 0.20707210898399353\n",
      "Epoch [467/1000], Loss MSE: 2.6270148754119873, Loss KL: 0.20763562619686127\n",
      "Epoch [468/1000], Loss MSE: 2.7767767906188965, Loss KL: 0.20819662511348724\n",
      "Epoch [469/1000], Loss MSE: 2.4994418621063232, Loss KL: 0.20874038338661194\n",
      "Epoch [470/1000], Loss MSE: 2.7550759315490723, Loss KL: 0.20930977165699005\n",
      "Epoch [471/1000], Loss MSE: 2.9172511100769043, Loss KL: 0.20985805988311768\n",
      "Epoch [472/1000], Loss MSE: 2.691800832748413, Loss KL: 0.21041634678840637\n",
      "Epoch [473/1000], Loss MSE: 2.641294002532959, Loss KL: 0.21095658838748932\n",
      "Epoch [474/1000], Loss MSE: 2.4775545597076416, Loss KL: 0.21153534948825836\n",
      "Epoch [475/1000], Loss MSE: 3.298737049102783, Loss KL: 0.21209827065467834\n",
      "Epoch [476/1000], Loss MSE: 2.4224793910980225, Loss KL: 0.21263357996940613\n",
      "Epoch [477/1000], Loss MSE: 3.588775157928467, Loss KL: 0.2131587266921997\n",
      "Epoch [478/1000], Loss MSE: 2.462280750274658, Loss KL: 0.21370920538902283\n",
      "Epoch [479/1000], Loss MSE: 2.459521532058716, Loss KL: 0.21425345540046692\n",
      "Epoch [480/1000], Loss MSE: 2.7375307083129883, Loss KL: 0.21481120586395264\n",
      "Epoch [481/1000], Loss MSE: 1.6824690103530884, Loss KL: 0.21537968516349792\n",
      "Epoch [482/1000], Loss MSE: 2.1734566688537598, Loss KL: 0.21594823896884918\n",
      "Epoch [483/1000], Loss MSE: 1.9375088214874268, Loss KL: 0.2165248990058899\n",
      "Epoch [484/1000], Loss MSE: 1.825238823890686, Loss KL: 0.21707850694656372\n",
      "Epoch [485/1000], Loss MSE: 2.5732169151306152, Loss KL: 0.2176135778427124\n",
      "Epoch [486/1000], Loss MSE: 3.0662527084350586, Loss KL: 0.2181599885225296\n",
      "Epoch [487/1000], Loss MSE: 2.491607189178467, Loss KL: 0.2187107801437378\n",
      "Epoch [488/1000], Loss MSE: 1.3899136781692505, Loss KL: 0.21926218271255493\n",
      "Epoch [489/1000], Loss MSE: 2.178379774093628, Loss KL: 0.21983171999454498\n",
      "Epoch [490/1000], Loss MSE: 2.466726779937744, Loss KL: 0.22035980224609375\n",
      "Epoch [491/1000], Loss MSE: 1.9557039737701416, Loss KL: 0.22094564139842987\n",
      "Epoch [492/1000], Loss MSE: 1.7703098058700562, Loss KL: 0.22150100767612457\n",
      "Epoch [493/1000], Loss MSE: 1.6342350244522095, Loss KL: 0.22204861044883728\n",
      "Epoch [494/1000], Loss MSE: 2.232165575027466, Loss KL: 0.22261950373649597\n",
      "Epoch [495/1000], Loss MSE: 3.6375503540039062, Loss KL: 0.22318053245544434\n",
      "Epoch [496/1000], Loss MSE: 3.356915235519409, Loss KL: 0.22373057901859283\n",
      "Epoch [497/1000], Loss MSE: 2.4294426441192627, Loss KL: 0.2242637574672699\n",
      "Epoch [498/1000], Loss MSE: 2.570582866668701, Loss KL: 0.22482314705848694\n",
      "Epoch [499/1000], Loss MSE: 1.6917359828948975, Loss KL: 0.22536970674991608\n",
      "Epoch [500/1000], Loss MSE: 2.334689140319824, Loss KL: 0.22592668235301971\n",
      "Epoch [501/1000], Loss MSE: 2.0388882160186768, Loss KL: 0.22649069130420685\n",
      "Epoch [502/1000], Loss MSE: 2.3999292850494385, Loss KL: 0.2270236313343048\n",
      "Epoch [503/1000], Loss MSE: 1.7633556127548218, Loss KL: 0.2275550812482834\n",
      "Epoch [504/1000], Loss MSE: 2.6474051475524902, Loss KL: 0.22809940576553345\n",
      "Epoch [505/1000], Loss MSE: 1.8122295141220093, Loss KL: 0.22863340377807617\n",
      "Epoch [506/1000], Loss MSE: 2.119495391845703, Loss KL: 0.22917424142360687\n",
      "Epoch [507/1000], Loss MSE: 3.063223361968994, Loss KL: 0.22973719239234924\n",
      "Epoch [508/1000], Loss MSE: 2.135530471801758, Loss KL: 0.23029311001300812\n",
      "Epoch [509/1000], Loss MSE: 2.223142385482788, Loss KL: 0.23083694279193878\n",
      "Epoch [510/1000], Loss MSE: 1.7509641647338867, Loss KL: 0.23137684166431427\n",
      "Epoch [511/1000], Loss MSE: 2.770228385925293, Loss KL: 0.23191878199577332\n",
      "Epoch [512/1000], Loss MSE: 2.1879401206970215, Loss KL: 0.23246468603610992\n",
      "Epoch [513/1000], Loss MSE: 2.23335599899292, Loss KL: 0.23301950097084045\n",
      "Epoch [514/1000], Loss MSE: 2.2079672813415527, Loss KL: 0.233563631772995\n",
      "Epoch [515/1000], Loss MSE: 1.7107559442520142, Loss KL: 0.23409484326839447\n",
      "Epoch [516/1000], Loss MSE: 2.6907763481140137, Loss KL: 0.2346263825893402\n",
      "Epoch [517/1000], Loss MSE: 1.2761693000793457, Loss KL: 0.235188826918602\n",
      "Epoch [518/1000], Loss MSE: 2.4901037216186523, Loss KL: 0.2357550412416458\n",
      "Epoch [519/1000], Loss MSE: 1.698625922203064, Loss KL: 0.23628397285938263\n",
      "Epoch [520/1000], Loss MSE: 1.6329323053359985, Loss KL: 0.2368282675743103\n",
      "Epoch [521/1000], Loss MSE: 2.10506010055542, Loss KL: 0.23737189173698425\n",
      "Epoch [522/1000], Loss MSE: 2.317080020904541, Loss KL: 0.2379293292760849\n",
      "Epoch [523/1000], Loss MSE: 2.1133034229278564, Loss KL: 0.23847408592700958\n",
      "Epoch [524/1000], Loss MSE: 1.839866042137146, Loss KL: 0.23900656402111053\n",
      "Epoch [525/1000], Loss MSE: 2.0900959968566895, Loss KL: 0.23951703310012817\n",
      "Epoch [526/1000], Loss MSE: 1.2182315587997437, Loss KL: 0.2400600016117096\n",
      "Epoch [527/1000], Loss MSE: 2.2439916133880615, Loss KL: 0.24060678482055664\n",
      "Epoch [528/1000], Loss MSE: 1.8382364511489868, Loss KL: 0.2411363273859024\n",
      "Epoch [529/1000], Loss MSE: 2.1704514026641846, Loss KL: 0.24166789650917053\n",
      "Epoch [530/1000], Loss MSE: 1.4173939228057861, Loss KL: 0.2422025352716446\n",
      "Epoch [531/1000], Loss MSE: 2.051518201828003, Loss KL: 0.24273741245269775\n",
      "Epoch [532/1000], Loss MSE: 2.209881067276001, Loss KL: 0.24325887858867645\n",
      "Epoch [533/1000], Loss MSE: 2.225940704345703, Loss KL: 0.24378187954425812\n",
      "Epoch [534/1000], Loss MSE: 2.038663864135742, Loss KL: 0.2443113476037979\n",
      "Epoch [535/1000], Loss MSE: 1.2546026706695557, Loss KL: 0.24486419558525085\n",
      "Epoch [536/1000], Loss MSE: 1.660249948501587, Loss KL: 0.24541069567203522\n",
      "Epoch [537/1000], Loss MSE: 1.8529716730117798, Loss KL: 0.2459360659122467\n",
      "Epoch [538/1000], Loss MSE: 1.6325056552886963, Loss KL: 0.24647416174411774\n",
      "Epoch [539/1000], Loss MSE: 1.6292917728424072, Loss KL: 0.24700778722763062\n",
      "Epoch [540/1000], Loss MSE: 1.083194375038147, Loss KL: 0.24749745428562164\n",
      "Epoch [541/1000], Loss MSE: 2.110170841217041, Loss KL: 0.24802270531654358\n",
      "Epoch [542/1000], Loss MSE: 2.011876106262207, Loss KL: 0.24856074154376984\n",
      "Epoch [543/1000], Loss MSE: 2.6176955699920654, Loss KL: 0.24913588166236877\n",
      "Epoch [544/1000], Loss MSE: 1.1155002117156982, Loss KL: 0.24969927966594696\n",
      "Epoch [545/1000], Loss MSE: 1.1823115348815918, Loss KL: 0.25024208426475525\n",
      "Epoch [546/1000], Loss MSE: 1.2103023529052734, Loss KL: 0.2507854402065277\n",
      "Epoch [547/1000], Loss MSE: 1.8543647527694702, Loss KL: 0.25133493542671204\n",
      "Epoch [548/1000], Loss MSE: 1.8329960107803345, Loss KL: 0.25185906887054443\n",
      "Epoch [549/1000], Loss MSE: 1.1953461170196533, Loss KL: 0.2523655891418457\n",
      "Epoch [550/1000], Loss MSE: 1.5921516418457031, Loss KL: 0.2528766393661499\n",
      "Epoch [551/1000], Loss MSE: 1.7494434118270874, Loss KL: 0.2534104287624359\n",
      "Epoch [552/1000], Loss MSE: 1.6917442083358765, Loss KL: 0.25394946336746216\n",
      "Epoch [553/1000], Loss MSE: 1.4966628551483154, Loss KL: 0.2544475793838501\n",
      "Epoch [554/1000], Loss MSE: 1.2668612003326416, Loss KL: 0.2549819350242615\n",
      "Epoch [555/1000], Loss MSE: 1.6573985815048218, Loss KL: 0.25548508763313293\n",
      "Epoch [556/1000], Loss MSE: 1.9192140102386475, Loss KL: 0.2559867799282074\n",
      "Epoch [557/1000], Loss MSE: 1.5592405796051025, Loss KL: 0.2565383315086365\n",
      "Epoch [558/1000], Loss MSE: 1.8392788171768188, Loss KL: 0.2570529878139496\n",
      "Epoch [559/1000], Loss MSE: 2.003854513168335, Loss KL: 0.257571816444397\n",
      "Epoch [560/1000], Loss MSE: 1.257122278213501, Loss KL: 0.2581026256084442\n",
      "Epoch [561/1000], Loss MSE: 1.671644926071167, Loss KL: 0.25860831141471863\n",
      "Epoch [562/1000], Loss MSE: 0.8866097331047058, Loss KL: 0.2591022253036499\n",
      "Epoch [563/1000], Loss MSE: 1.7580410242080688, Loss KL: 0.2596314549446106\n",
      "Epoch [564/1000], Loss MSE: 1.41266667842865, Loss KL: 0.26016056537628174\n",
      "Epoch [565/1000], Loss MSE: 1.8573719263076782, Loss KL: 0.2606661319732666\n",
      "Epoch [566/1000], Loss MSE: 1.269166111946106, Loss KL: 0.2611858546733856\n",
      "Epoch [567/1000], Loss MSE: 1.85581374168396, Loss KL: 0.2617110311985016\n",
      "Epoch [568/1000], Loss MSE: 1.4863357543945312, Loss KL: 0.26218050718307495\n",
      "Epoch [569/1000], Loss MSE: 1.2250880002975464, Loss KL: 0.2626841962337494\n",
      "Epoch [570/1000], Loss MSE: 1.2382397651672363, Loss KL: 0.2631840705871582\n",
      "Epoch [571/1000], Loss MSE: 0.9213861227035522, Loss KL: 0.2636682987213135\n",
      "Epoch [572/1000], Loss MSE: 1.5547412633895874, Loss KL: 0.2641531229019165\n",
      "Epoch [573/1000], Loss MSE: 1.4791982173919678, Loss KL: 0.2646615207195282\n",
      "Epoch [574/1000], Loss MSE: 1.4678199291229248, Loss KL: 0.2651689946651459\n",
      "Epoch [575/1000], Loss MSE: 1.4767087697982788, Loss KL: 0.26566076278686523\n",
      "Epoch [576/1000], Loss MSE: 1.307759165763855, Loss KL: 0.26616939902305603\n",
      "Epoch [577/1000], Loss MSE: 1.2207176685333252, Loss KL: 0.2666667699813843\n",
      "Epoch [578/1000], Loss MSE: 1.0906702280044556, Loss KL: 0.26717960834503174\n",
      "Epoch [579/1000], Loss MSE: 0.9914764165878296, Loss KL: 0.26768243312835693\n",
      "Epoch [580/1000], Loss MSE: 0.8500629663467407, Loss KL: 0.2681647837162018\n",
      "Epoch [581/1000], Loss MSE: 1.4608973264694214, Loss KL: 0.2686636447906494\n",
      "Epoch [582/1000], Loss MSE: 1.3857468366622925, Loss KL: 0.2691527307033539\n",
      "Epoch [583/1000], Loss MSE: 1.2989548444747925, Loss KL: 0.2696072459220886\n",
      "Epoch [584/1000], Loss MSE: 1.6010860204696655, Loss KL: 0.2701222002506256\n",
      "Epoch [585/1000], Loss MSE: 1.794854760169983, Loss KL: 0.2706019878387451\n",
      "Epoch [586/1000], Loss MSE: 1.0537264347076416, Loss KL: 0.27106404304504395\n",
      "Epoch [587/1000], Loss MSE: 1.233783483505249, Loss KL: 0.2715505361557007\n",
      "Epoch [588/1000], Loss MSE: 1.3620860576629639, Loss KL: 0.2720186412334442\n",
      "Epoch [589/1000], Loss MSE: 1.2103089094161987, Loss KL: 0.27249905467033386\n",
      "Epoch [590/1000], Loss MSE: 1.0485063791275024, Loss KL: 0.27295994758605957\n",
      "Epoch [591/1000], Loss MSE: 1.071873426437378, Loss KL: 0.27342864871025085\n",
      "Epoch [592/1000], Loss MSE: 1.1500756740570068, Loss KL: 0.27390459179878235\n",
      "Epoch [593/1000], Loss MSE: 1.4604142904281616, Loss KL: 0.2744027376174927\n",
      "Epoch [594/1000], Loss MSE: 1.3439252376556396, Loss KL: 0.2748802900314331\n",
      "Epoch [595/1000], Loss MSE: 0.9902693033218384, Loss KL: 0.2753816545009613\n",
      "Epoch [596/1000], Loss MSE: 1.4632518291473389, Loss KL: 0.27584630250930786\n",
      "Epoch [597/1000], Loss MSE: 1.4197962284088135, Loss KL: 0.2763053774833679\n",
      "Epoch [598/1000], Loss MSE: 1.3242543935775757, Loss KL: 0.2767883241176605\n",
      "Epoch [599/1000], Loss MSE: 1.1409063339233398, Loss KL: 0.2772756516933441\n",
      "Epoch [600/1000], Loss MSE: 1.0703827142715454, Loss KL: 0.27773717045783997\n",
      "Epoch [601/1000], Loss MSE: 1.2961013317108154, Loss KL: 0.2782118618488312\n",
      "Epoch [602/1000], Loss MSE: 0.9242690205574036, Loss KL: 0.2786601781845093\n",
      "Epoch [603/1000], Loss MSE: 1.445053219795227, Loss KL: 0.27913957834243774\n",
      "Epoch [604/1000], Loss MSE: 1.051313877105713, Loss KL: 0.27960237860679626\n",
      "Epoch [605/1000], Loss MSE: 0.9290739297866821, Loss KL: 0.2800765931606293\n",
      "Epoch [606/1000], Loss MSE: 1.1431419849395752, Loss KL: 0.28055569529533386\n",
      "Epoch [607/1000], Loss MSE: 1.5043208599090576, Loss KL: 0.2810312509536743\n",
      "Epoch [608/1000], Loss MSE: 0.7554246187210083, Loss KL: 0.28147175908088684\n",
      "Epoch [609/1000], Loss MSE: 1.3125776052474976, Loss KL: 0.28190916776657104\n",
      "Epoch [610/1000], Loss MSE: 1.4003039598464966, Loss KL: 0.2823711037635803\n",
      "Epoch [611/1000], Loss MSE: 1.1208258867263794, Loss KL: 0.28281450271606445\n",
      "Epoch [612/1000], Loss MSE: 1.0549705028533936, Loss KL: 0.2832755148410797\n",
      "Epoch [613/1000], Loss MSE: 0.920799732208252, Loss KL: 0.28373244404792786\n",
      "Epoch [614/1000], Loss MSE: 1.2438281774520874, Loss KL: 0.2841527760028839\n",
      "Epoch [615/1000], Loss MSE: 1.3250502347946167, Loss KL: 0.28462812304496765\n",
      "Epoch [616/1000], Loss MSE: 1.0375101566314697, Loss KL: 0.2850923538208008\n",
      "Epoch [617/1000], Loss MSE: 0.8878108859062195, Loss KL: 0.28555795550346375\n",
      "Epoch [618/1000], Loss MSE: 1.3984482288360596, Loss KL: 0.2859737277030945\n",
      "Epoch [619/1000], Loss MSE: 0.9190897941589355, Loss KL: 0.2864317297935486\n",
      "Epoch [620/1000], Loss MSE: 1.1656310558319092, Loss KL: 0.28682568669319153\n",
      "Epoch [621/1000], Loss MSE: 0.9716631174087524, Loss KL: 0.2872498333454132\n",
      "Epoch [622/1000], Loss MSE: 0.9706381559371948, Loss KL: 0.2876889109611511\n",
      "Epoch [623/1000], Loss MSE: 0.9566447138786316, Loss KL: 0.2881515622138977\n",
      "Epoch [624/1000], Loss MSE: 0.8809057474136353, Loss KL: 0.28857797384262085\n",
      "Epoch [625/1000], Loss MSE: 0.8131521940231323, Loss KL: 0.28898680210113525\n",
      "Epoch [626/1000], Loss MSE: 1.3430911302566528, Loss KL: 0.28942495584487915\n",
      "Epoch [627/1000], Loss MSE: 0.7398358583450317, Loss KL: 0.2898091673851013\n",
      "Epoch [628/1000], Loss MSE: 0.9265419244766235, Loss KL: 0.29023948311805725\n",
      "Epoch [629/1000], Loss MSE: 1.084688425064087, Loss KL: 0.2906475365161896\n",
      "Epoch [630/1000], Loss MSE: 1.3092355728149414, Loss KL: 0.29107096791267395\n",
      "Epoch [631/1000], Loss MSE: 1.1285680532455444, Loss KL: 0.29149773716926575\n",
      "Epoch [632/1000], Loss MSE: 1.0894418954849243, Loss KL: 0.29189029335975647\n",
      "Epoch [633/1000], Loss MSE: 0.7839480638504028, Loss KL: 0.29233038425445557\n",
      "Epoch [634/1000], Loss MSE: 1.1436915397644043, Loss KL: 0.2927524447441101\n",
      "Epoch [635/1000], Loss MSE: 1.1018040180206299, Loss KL: 0.29315075278282166\n",
      "Epoch [636/1000], Loss MSE: 1.0100526809692383, Loss KL: 0.2935738265514374\n",
      "Epoch [637/1000], Loss MSE: 0.8100214004516602, Loss KL: 0.29401451349258423\n",
      "Epoch [638/1000], Loss MSE: 0.8359947204589844, Loss KL: 0.29441869258880615\n",
      "Epoch [639/1000], Loss MSE: 0.9455066919326782, Loss KL: 0.2948046624660492\n",
      "Epoch [640/1000], Loss MSE: 0.9990202188491821, Loss KL: 0.2952042520046234\n",
      "Epoch [641/1000], Loss MSE: 1.2070422172546387, Loss KL: 0.2955854833126068\n",
      "Epoch [642/1000], Loss MSE: 0.7129535675048828, Loss KL: 0.2960088849067688\n",
      "Epoch [643/1000], Loss MSE: 0.821839451789856, Loss KL: 0.29642269015312195\n",
      "Epoch [644/1000], Loss MSE: 0.8103213310241699, Loss KL: 0.2968228757381439\n",
      "Epoch [645/1000], Loss MSE: 1.1967953443527222, Loss KL: 0.29720959067344666\n",
      "Epoch [646/1000], Loss MSE: 0.5409137606620789, Loss KL: 0.2976062595844269\n",
      "Epoch [647/1000], Loss MSE: 1.1676514148712158, Loss KL: 0.2979617416858673\n",
      "Epoch [648/1000], Loss MSE: 0.9755750894546509, Loss KL: 0.2983347475528717\n",
      "Epoch [649/1000], Loss MSE: 1.117789626121521, Loss KL: 0.29872599244117737\n",
      "Epoch [650/1000], Loss MSE: 0.9597377777099609, Loss KL: 0.2991511821746826\n",
      "Epoch [651/1000], Loss MSE: 1.1012589931488037, Loss KL: 0.2995694875717163\n",
      "Epoch [652/1000], Loss MSE: 1.0506856441497803, Loss KL: 0.29992398619651794\n",
      "Epoch [653/1000], Loss MSE: 0.750471293926239, Loss KL: 0.3003237247467041\n",
      "Epoch [654/1000], Loss MSE: 1.2556545734405518, Loss KL: 0.3006778359413147\n",
      "Epoch [655/1000], Loss MSE: 0.9856632351875305, Loss KL: 0.30107182264328003\n",
      "Epoch [656/1000], Loss MSE: 0.7343658208847046, Loss KL: 0.3014585077762604\n",
      "Epoch [657/1000], Loss MSE: 0.8868597745895386, Loss KL: 0.30179765820503235\n",
      "Epoch [658/1000], Loss MSE: 0.9376751780509949, Loss KL: 0.3021785318851471\n",
      "Epoch [659/1000], Loss MSE: 1.0419367551803589, Loss KL: 0.3025495707988739\n",
      "Epoch [660/1000], Loss MSE: 0.9945107698440552, Loss KL: 0.30296754837036133\n",
      "Epoch [661/1000], Loss MSE: 0.5939748883247375, Loss KL: 0.3033193051815033\n",
      "Epoch [662/1000], Loss MSE: 0.871557891368866, Loss KL: 0.303661584854126\n",
      "Epoch [663/1000], Loss MSE: 0.9988929629325867, Loss KL: 0.3040580153465271\n",
      "Epoch [664/1000], Loss MSE: 1.2245689630508423, Loss KL: 0.3044132888317108\n",
      "Epoch [665/1000], Loss MSE: 0.653516411781311, Loss KL: 0.30478882789611816\n",
      "Epoch [666/1000], Loss MSE: 0.9773851633071899, Loss KL: 0.30511391162872314\n",
      "Epoch [667/1000], Loss MSE: 0.6990114450454712, Loss KL: 0.3055066168308258\n",
      "Epoch [668/1000], Loss MSE: 0.6437134742736816, Loss KL: 0.30582305788993835\n",
      "Epoch [669/1000], Loss MSE: 1.0008907318115234, Loss KL: 0.30619722604751587\n",
      "Epoch [670/1000], Loss MSE: 0.9145783185958862, Loss KL: 0.3065667450428009\n",
      "Epoch [671/1000], Loss MSE: 0.9520982503890991, Loss KL: 0.30692484974861145\n",
      "Epoch [672/1000], Loss MSE: 1.0721120834350586, Loss KL: 0.30727627873420715\n",
      "Epoch [673/1000], Loss MSE: 0.958171010017395, Loss KL: 0.3076002895832062\n",
      "Epoch [674/1000], Loss MSE: 0.6710038781166077, Loss KL: 0.30793365836143494\n",
      "Epoch [675/1000], Loss MSE: 0.2920108735561371, Loss KL: 0.3082866370677948\n",
      "Epoch [676/1000], Loss MSE: 0.9722498655319214, Loss KL: 0.30862459540367126\n",
      "Epoch [677/1000], Loss MSE: 1.0058104991912842, Loss KL: 0.3089464008808136\n",
      "Epoch [678/1000], Loss MSE: 0.704218327999115, Loss KL: 0.3092764914035797\n",
      "Epoch [679/1000], Loss MSE: 0.630868673324585, Loss KL: 0.3095932900905609\n",
      "Epoch [680/1000], Loss MSE: 0.9031465649604797, Loss KL: 0.3099551498889923\n",
      "Epoch [681/1000], Loss MSE: 0.9392127990722656, Loss KL: 0.31029394268989563\n",
      "Epoch [682/1000], Loss MSE: 0.8630512952804565, Loss KL: 0.31055402755737305\n",
      "Epoch [683/1000], Loss MSE: 0.87987220287323, Loss KL: 0.3108798861503601\n",
      "Epoch [684/1000], Loss MSE: 0.8735092878341675, Loss KL: 0.3112456202507019\n",
      "Epoch [685/1000], Loss MSE: 0.9387698173522949, Loss KL: 0.3115079998970032\n",
      "Epoch [686/1000], Loss MSE: 0.7490497827529907, Loss KL: 0.3117823600769043\n",
      "Epoch [687/1000], Loss MSE: 0.8526795506477356, Loss KL: 0.3121250867843628\n",
      "Epoch [688/1000], Loss MSE: 0.6247894167900085, Loss KL: 0.3124527335166931\n",
      "Epoch [689/1000], Loss MSE: 0.5127724409103394, Loss KL: 0.31276842951774597\n",
      "Epoch [690/1000], Loss MSE: 0.9581314921379089, Loss KL: 0.31312477588653564\n",
      "Epoch [691/1000], Loss MSE: 0.9341486096382141, Loss KL: 0.3134094476699829\n",
      "Epoch [692/1000], Loss MSE: 0.6136666536331177, Loss KL: 0.31370458006858826\n",
      "Epoch [693/1000], Loss MSE: 0.8845297694206238, Loss KL: 0.3140624165534973\n",
      "Epoch [694/1000], Loss MSE: 0.5754832625389099, Loss KL: 0.314334899187088\n",
      "Epoch [695/1000], Loss MSE: 1.1206505298614502, Loss KL: 0.31463563442230225\n",
      "Epoch [696/1000], Loss MSE: 0.6965645551681519, Loss KL: 0.3149488866329193\n",
      "Epoch [697/1000], Loss MSE: 0.7665159702301025, Loss KL: 0.3152560591697693\n",
      "Epoch [698/1000], Loss MSE: 0.8707818984985352, Loss KL: 0.31557103991508484\n",
      "Epoch [699/1000], Loss MSE: 0.6435896158218384, Loss KL: 0.31589123606681824\n",
      "Epoch [700/1000], Loss MSE: 0.9131506085395813, Loss KL: 0.31618645787239075\n",
      "Epoch [701/1000], Loss MSE: 0.5724099278450012, Loss KL: 0.3164897561073303\n",
      "Epoch [702/1000], Loss MSE: 1.0366815328598022, Loss KL: 0.31673911213874817\n",
      "Epoch [703/1000], Loss MSE: 0.8264071345329285, Loss KL: 0.31701308488845825\n",
      "Epoch [704/1000], Loss MSE: 0.64703768491745, Loss KL: 0.3173183798789978\n",
      "Epoch [705/1000], Loss MSE: 0.5669924020767212, Loss KL: 0.3175673484802246\n",
      "Epoch [706/1000], Loss MSE: 0.5413669347763062, Loss KL: 0.3178792893886566\n",
      "Epoch [707/1000], Loss MSE: 0.47728878259658813, Loss KL: 0.3181550204753876\n",
      "Epoch [708/1000], Loss MSE: 0.7932614088058472, Loss KL: 0.318415105342865\n",
      "Epoch [709/1000], Loss MSE: 0.7971601486206055, Loss KL: 0.3187035918235779\n",
      "Epoch [710/1000], Loss MSE: 0.7122405767440796, Loss KL: 0.3189503252506256\n",
      "Epoch [711/1000], Loss MSE: 0.8778656125068665, Loss KL: 0.3192153871059418\n",
      "Epoch [712/1000], Loss MSE: 0.749313235282898, Loss KL: 0.31947198510169983\n",
      "Epoch [713/1000], Loss MSE: 0.821958065032959, Loss KL: 0.31976333260536194\n",
      "Epoch [714/1000], Loss MSE: 0.8011434674263, Loss KL: 0.3200165331363678\n",
      "Epoch [715/1000], Loss MSE: 0.9331164360046387, Loss KL: 0.320285826921463\n",
      "Epoch [716/1000], Loss MSE: 0.5622689127922058, Loss KL: 0.32053646445274353\n",
      "Epoch [717/1000], Loss MSE: 0.7822786569595337, Loss KL: 0.3208167850971222\n",
      "Epoch [718/1000], Loss MSE: 0.5522056818008423, Loss KL: 0.32105422019958496\n",
      "Epoch [719/1000], Loss MSE: 0.7836562395095825, Loss KL: 0.3213331997394562\n",
      "Epoch [720/1000], Loss MSE: 0.5769521594047546, Loss KL: 0.32160496711730957\n",
      "Epoch [721/1000], Loss MSE: 0.5572271943092346, Loss KL: 0.3218725025653839\n",
      "Epoch [722/1000], Loss MSE: 0.4358305335044861, Loss KL: 0.3221374750137329\n",
      "Epoch [723/1000], Loss MSE: 0.9350861310958862, Loss KL: 0.322394460439682\n",
      "Epoch [724/1000], Loss MSE: 0.8641446232795715, Loss KL: 0.32257193326950073\n",
      "Epoch [725/1000], Loss MSE: 1.0803574323654175, Loss KL: 0.3228447735309601\n",
      "Epoch [726/1000], Loss MSE: 0.6221676468849182, Loss KL: 0.3230621814727783\n",
      "Epoch [727/1000], Loss MSE: 0.5350289344787598, Loss KL: 0.32330402731895447\n",
      "Epoch [728/1000], Loss MSE: 1.1110212802886963, Loss KL: 0.3235395550727844\n",
      "Epoch [729/1000], Loss MSE: 0.6142883896827698, Loss KL: 0.3237868845462799\n",
      "Epoch [730/1000], Loss MSE: 0.5611961483955383, Loss KL: 0.3240209221839905\n",
      "Epoch [731/1000], Loss MSE: 0.6216696500778198, Loss KL: 0.32427269220352173\n",
      "Epoch [732/1000], Loss MSE: 0.4800214171409607, Loss KL: 0.3245353102684021\n",
      "Epoch [733/1000], Loss MSE: 0.8643724322319031, Loss KL: 0.32475942373275757\n",
      "Epoch [734/1000], Loss MSE: 0.9055694341659546, Loss KL: 0.32499170303344727\n",
      "Epoch [735/1000], Loss MSE: 0.7869419455528259, Loss KL: 0.32525041699409485\n",
      "Epoch [736/1000], Loss MSE: 0.4206327795982361, Loss KL: 0.3254415690898895\n",
      "Epoch [737/1000], Loss MSE: 0.5221797227859497, Loss KL: 0.3256750702857971\n",
      "Epoch [738/1000], Loss MSE: 1.1931746006011963, Loss KL: 0.32591795921325684\n",
      "Epoch [739/1000], Loss MSE: 0.6073171496391296, Loss KL: 0.3261287808418274\n",
      "Epoch [740/1000], Loss MSE: 1.0484974384307861, Loss KL: 0.3263489305973053\n",
      "Epoch [741/1000], Loss MSE: 1.2620995044708252, Loss KL: 0.3266024589538574\n",
      "Epoch [742/1000], Loss MSE: 0.6029963493347168, Loss KL: 0.3267969489097595\n",
      "Epoch [743/1000], Loss MSE: 0.826555609703064, Loss KL: 0.32698625326156616\n",
      "Epoch [744/1000], Loss MSE: 0.6819566488265991, Loss KL: 0.32719218730926514\n",
      "Epoch [745/1000], Loss MSE: 0.5158640146255493, Loss KL: 0.32741352915763855\n",
      "Epoch [746/1000], Loss MSE: 0.9682475924491882, Loss KL: 0.32767587900161743\n",
      "Epoch [747/1000], Loss MSE: 0.5068434476852417, Loss KL: 0.32792067527770996\n",
      "Epoch [748/1000], Loss MSE: 0.7805523872375488, Loss KL: 0.3280700743198395\n",
      "Epoch [749/1000], Loss MSE: 0.33383864164352417, Loss KL: 0.32828235626220703\n",
      "Epoch [750/1000], Loss MSE: 0.6425265073776245, Loss KL: 0.3285309970378876\n",
      "Epoch [751/1000], Loss MSE: 0.396330326795578, Loss KL: 0.3287685215473175\n",
      "Epoch [752/1000], Loss MSE: 0.586678147315979, Loss KL: 0.3289993107318878\n",
      "Epoch [753/1000], Loss MSE: 0.28112122416496277, Loss KL: 0.32920193672180176\n",
      "Epoch [754/1000], Loss MSE: 0.5753566026687622, Loss KL: 0.3294486701488495\n",
      "Epoch [755/1000], Loss MSE: 0.6046941876411438, Loss KL: 0.3296748995780945\n",
      "Epoch [756/1000], Loss MSE: 0.8846400380134583, Loss KL: 0.32984060049057007\n",
      "Epoch [757/1000], Loss MSE: 0.9622923731803894, Loss KL: 0.33002716302871704\n",
      "Epoch [758/1000], Loss MSE: 0.3280438780784607, Loss KL: 0.33017924427986145\n",
      "Epoch [759/1000], Loss MSE: 0.8989585041999817, Loss KL: 0.3303925096988678\n",
      "Epoch [760/1000], Loss MSE: 0.4904530942440033, Loss KL: 0.33057957887649536\n",
      "Epoch [761/1000], Loss MSE: 0.5292243361473083, Loss KL: 0.3307744860649109\n",
      "Epoch [762/1000], Loss MSE: 0.5346004962921143, Loss KL: 0.3309175670146942\n",
      "Epoch [763/1000], Loss MSE: 0.5043578147888184, Loss KL: 0.331152081489563\n",
      "Epoch [764/1000], Loss MSE: 0.7647382020950317, Loss KL: 0.3313509225845337\n",
      "Epoch [765/1000], Loss MSE: 0.9184867143630981, Loss KL: 0.33153125643730164\n",
      "Epoch [766/1000], Loss MSE: 0.45677027106285095, Loss KL: 0.33171650767326355\n",
      "Epoch [767/1000], Loss MSE: 0.7724484205245972, Loss KL: 0.3319455087184906\n",
      "Epoch [768/1000], Loss MSE: 0.7046960592269897, Loss KL: 0.3321588635444641\n",
      "Epoch [769/1000], Loss MSE: 0.6889471411705017, Loss KL: 0.3323485851287842\n",
      "Epoch [770/1000], Loss MSE: 0.49856144189834595, Loss KL: 0.33249375224113464\n",
      "Epoch [771/1000], Loss MSE: 0.6546346545219421, Loss KL: 0.33272603154182434\n",
      "Epoch [772/1000], Loss MSE: 0.677132248878479, Loss KL: 0.33293086290359497\n",
      "Epoch [773/1000], Loss MSE: 0.9836082458496094, Loss KL: 0.3331224024295807\n",
      "Epoch [774/1000], Loss MSE: 0.43830814957618713, Loss KL: 0.3332790434360504\n",
      "Epoch [775/1000], Loss MSE: 0.4280993342399597, Loss KL: 0.33347082138061523\n",
      "Epoch [776/1000], Loss MSE: 0.7724646925926208, Loss KL: 0.33366668224334717\n",
      "Epoch [777/1000], Loss MSE: 0.5927250981330872, Loss KL: 0.33384835720062256\n",
      "Epoch [778/1000], Loss MSE: 0.6057004332542419, Loss KL: 0.3340153992176056\n",
      "Epoch [779/1000], Loss MSE: 0.3820527493953705, Loss KL: 0.33416107296943665\n",
      "Epoch [780/1000], Loss MSE: 0.9672384262084961, Loss KL: 0.33434727787971497\n",
      "Epoch [781/1000], Loss MSE: 0.5742758512496948, Loss KL: 0.33453506231307983\n",
      "Epoch [782/1000], Loss MSE: 0.9730385541915894, Loss KL: 0.33475425839424133\n",
      "Epoch [783/1000], Loss MSE: 0.555629312992096, Loss KL: 0.3349154591560364\n",
      "Epoch [784/1000], Loss MSE: 0.5287780165672302, Loss KL: 0.33506789803504944\n",
      "Epoch [785/1000], Loss MSE: 0.6662351489067078, Loss KL: 0.335211843252182\n",
      "Epoch [786/1000], Loss MSE: 0.258612722158432, Loss KL: 0.335407018661499\n",
      "Epoch [787/1000], Loss MSE: 0.9988112449645996, Loss KL: 0.33552637696266174\n",
      "Epoch [788/1000], Loss MSE: 0.804195761680603, Loss KL: 0.33570531010627747\n",
      "Epoch [789/1000], Loss MSE: 0.8217395544052124, Loss KL: 0.33591222763061523\n",
      "Epoch [790/1000], Loss MSE: 0.5893694758415222, Loss KL: 0.3360232710838318\n",
      "Epoch [791/1000], Loss MSE: 0.9096049070358276, Loss KL: 0.3361940085887909\n",
      "Epoch [792/1000], Loss MSE: 0.513245701789856, Loss KL: 0.3363815248012543\n",
      "Epoch [793/1000], Loss MSE: 0.576010525226593, Loss KL: 0.33650878071784973\n",
      "Epoch [794/1000], Loss MSE: 0.6374038457870483, Loss KL: 0.3366543650627136\n",
      "Epoch [795/1000], Loss MSE: 1.1475651264190674, Loss KL: 0.3367955982685089\n",
      "Epoch [796/1000], Loss MSE: 0.476127564907074, Loss KL: 0.3370092511177063\n",
      "Epoch [797/1000], Loss MSE: 0.7623873353004456, Loss KL: 0.33712223172187805\n",
      "Epoch [798/1000], Loss MSE: 0.33517056703567505, Loss KL: 0.33726489543914795\n",
      "Epoch [799/1000], Loss MSE: 0.559441089630127, Loss KL: 0.3374292850494385\n",
      "Epoch [800/1000], Loss MSE: 0.519014298915863, Loss KL: 0.3375271260738373\n",
      "Epoch [801/1000], Loss MSE: 0.582361102104187, Loss KL: 0.3376869559288025\n",
      "Epoch [802/1000], Loss MSE: 0.936292827129364, Loss KL: 0.3378449082374573\n",
      "Epoch [803/1000], Loss MSE: 0.5538522005081177, Loss KL: 0.33800026774406433\n",
      "Epoch [804/1000], Loss MSE: 0.8749591708183289, Loss KL: 0.3382159173488617\n",
      "Epoch [805/1000], Loss MSE: 1.1388260126113892, Loss KL: 0.33836686611175537\n",
      "Epoch [806/1000], Loss MSE: 0.4123244881629944, Loss KL: 0.3385032117366791\n",
      "Epoch [807/1000], Loss MSE: 0.36267518997192383, Loss KL: 0.3386272192001343\n",
      "Epoch [808/1000], Loss MSE: 0.3326686918735504, Loss KL: 0.3387494385242462\n",
      "Epoch [809/1000], Loss MSE: 0.72332763671875, Loss KL: 0.3388746976852417\n",
      "Epoch [810/1000], Loss MSE: 0.7417635321617126, Loss KL: 0.33905819058418274\n",
      "Epoch [811/1000], Loss MSE: 0.42250996828079224, Loss KL: 0.33921149373054504\n",
      "Epoch [812/1000], Loss MSE: 0.2835412621498108, Loss KL: 0.33930477499961853\n",
      "Epoch [813/1000], Loss MSE: 0.42469891905784607, Loss KL: 0.33944493532180786\n",
      "Epoch [814/1000], Loss MSE: 0.6069427728652954, Loss KL: 0.3395679295063019\n",
      "Epoch [815/1000], Loss MSE: 0.3227023780345917, Loss KL: 0.3396913409233093\n",
      "Epoch [816/1000], Loss MSE: 0.5716763734817505, Loss KL: 0.3398585021495819\n",
      "Epoch [817/1000], Loss MSE: 0.3171427249908447, Loss KL: 0.33998817205429077\n",
      "Epoch [818/1000], Loss MSE: 0.4411807954311371, Loss KL: 0.34008729457855225\n",
      "Epoch [819/1000], Loss MSE: 0.5155413746833801, Loss KL: 0.3402252793312073\n",
      "Epoch [820/1000], Loss MSE: 0.59319007396698, Loss KL: 0.3403332531452179\n",
      "Epoch [821/1000], Loss MSE: 0.5131067037582397, Loss KL: 0.3404487371444702\n",
      "Epoch [822/1000], Loss MSE: 0.9503476023674011, Loss KL: 0.34055250883102417\n",
      "Epoch [823/1000], Loss MSE: 0.4212287366390228, Loss KL: 0.3406831920146942\n",
      "Epoch [824/1000], Loss MSE: 0.5232791900634766, Loss KL: 0.34080958366394043\n",
      "Epoch [825/1000], Loss MSE: 1.1053014993667603, Loss KL: 0.3409375846385956\n",
      "Epoch [826/1000], Loss MSE: 1.03696870803833, Loss KL: 0.3410432040691376\n",
      "Epoch [827/1000], Loss MSE: 0.34330064058303833, Loss KL: 0.3411239683628082\n",
      "Epoch [828/1000], Loss MSE: 0.3162539303302765, Loss KL: 0.3412784934043884\n",
      "Epoch [829/1000], Loss MSE: 0.3565261960029602, Loss KL: 0.34138351678848267\n",
      "Epoch [830/1000], Loss MSE: 0.8871868252754211, Loss KL: 0.3415173590183258\n",
      "Epoch [831/1000], Loss MSE: 0.7717615365982056, Loss KL: 0.3416048586368561\n",
      "Epoch [832/1000], Loss MSE: 0.9130029678344727, Loss KL: 0.34172719717025757\n",
      "Epoch [833/1000], Loss MSE: 0.9316444396972656, Loss KL: 0.3417893052101135\n",
      "Epoch [834/1000], Loss MSE: 0.7989914417266846, Loss KL: 0.3419235348701477\n",
      "Epoch [835/1000], Loss MSE: 0.40204253792762756, Loss KL: 0.34203580021858215\n",
      "Epoch [836/1000], Loss MSE: 0.7581608891487122, Loss KL: 0.342151939868927\n",
      "Epoch [837/1000], Loss MSE: 0.47217339277267456, Loss KL: 0.3422639071941376\n",
      "Epoch [838/1000], Loss MSE: 0.6562387347221375, Loss KL: 0.3423488438129425\n",
      "Epoch [839/1000], Loss MSE: 0.36934781074523926, Loss KL: 0.342460036277771\n",
      "Epoch [840/1000], Loss MSE: 1.1246814727783203, Loss KL: 0.34253740310668945\n",
      "Epoch [841/1000], Loss MSE: 0.5718915462493896, Loss KL: 0.3426540791988373\n",
      "Epoch [842/1000], Loss MSE: 0.5143457055091858, Loss KL: 0.34279942512512207\n",
      "Epoch [843/1000], Loss MSE: 1.0933507680892944, Loss KL: 0.3429161310195923\n",
      "Epoch [844/1000], Loss MSE: 0.8371126055717468, Loss KL: 0.34300824999809265\n",
      "Epoch [845/1000], Loss MSE: 0.35164108872413635, Loss KL: 0.34309911727905273\n",
      "Epoch [846/1000], Loss MSE: 0.3453157842159271, Loss KL: 0.34321466088294983\n",
      "Epoch [847/1000], Loss MSE: 0.4916127324104309, Loss KL: 0.34333038330078125\n",
      "Epoch [848/1000], Loss MSE: 0.7933664321899414, Loss KL: 0.34347832202911377\n",
      "Epoch [849/1000], Loss MSE: 0.5679146647453308, Loss KL: 0.34355926513671875\n",
      "Epoch [850/1000], Loss MSE: 0.6790446043014526, Loss KL: 0.3436558246612549\n",
      "Epoch [851/1000], Loss MSE: 1.0860931873321533, Loss KL: 0.3437444567680359\n",
      "Epoch [852/1000], Loss MSE: 0.5344237089157104, Loss KL: 0.34386223554611206\n",
      "Epoch [853/1000], Loss MSE: 0.5429977774620056, Loss KL: 0.34395065903663635\n",
      "Epoch [854/1000], Loss MSE: 0.29452866315841675, Loss KL: 0.3440755307674408\n",
      "Epoch [855/1000], Loss MSE: 1.056105136871338, Loss KL: 0.34418758749961853\n",
      "Epoch [856/1000], Loss MSE: 0.5286715626716614, Loss KL: 0.34425440430641174\n",
      "Epoch [857/1000], Loss MSE: 0.5288880467414856, Loss KL: 0.3443692922592163\n",
      "Epoch [858/1000], Loss MSE: 0.7507839798927307, Loss KL: 0.3444778621196747\n",
      "Epoch [859/1000], Loss MSE: 0.6505654454231262, Loss KL: 0.3445611596107483\n",
      "Epoch [860/1000], Loss MSE: 0.3530595898628235, Loss KL: 0.34467512369155884\n",
      "Epoch [861/1000], Loss MSE: 0.20807373523712158, Loss KL: 0.34477901458740234\n",
      "Epoch [862/1000], Loss MSE: 0.8355826139450073, Loss KL: 0.3448730707168579\n",
      "Epoch [863/1000], Loss MSE: 0.2723383605480194, Loss KL: 0.34498175978660583\n",
      "Epoch [864/1000], Loss MSE: 1.0774041414260864, Loss KL: 0.34508341550827026\n",
      "Epoch [865/1000], Loss MSE: 0.8181584477424622, Loss KL: 0.3451996445655823\n",
      "Epoch [866/1000], Loss MSE: 0.24145357310771942, Loss KL: 0.34529414772987366\n",
      "Epoch [867/1000], Loss MSE: 0.39739587903022766, Loss KL: 0.34539058804512024\n",
      "Epoch [868/1000], Loss MSE: 0.3875132203102112, Loss KL: 0.3454660177230835\n",
      "Epoch [869/1000], Loss MSE: 0.24460168182849884, Loss KL: 0.34559422731399536\n",
      "Epoch [870/1000], Loss MSE: 0.3855307102203369, Loss KL: 0.3456985354423523\n",
      "Epoch [871/1000], Loss MSE: 0.24477486312389374, Loss KL: 0.34578680992126465\n",
      "Epoch [872/1000], Loss MSE: 1.1563786268234253, Loss KL: 0.345892071723938\n",
      "Epoch [873/1000], Loss MSE: 0.5122765302658081, Loss KL: 0.3459528386592865\n",
      "Epoch [874/1000], Loss MSE: 0.5320900678634644, Loss KL: 0.3460753262042999\n",
      "Epoch [875/1000], Loss MSE: 0.46658840775489807, Loss KL: 0.3461204171180725\n",
      "Epoch [876/1000], Loss MSE: 0.30657222867012024, Loss KL: 0.34621259570121765\n",
      "Epoch [877/1000], Loss MSE: 0.2808491289615631, Loss KL: 0.34628742933273315\n",
      "Epoch [878/1000], Loss MSE: 0.41289687156677246, Loss KL: 0.3464338183403015\n",
      "Epoch [879/1000], Loss MSE: 1.2287781238555908, Loss KL: 0.34657588601112366\n",
      "Epoch [880/1000], Loss MSE: 1.2731130123138428, Loss KL: 0.34664416313171387\n",
      "Epoch [881/1000], Loss MSE: 0.8009811639785767, Loss KL: 0.3467172384262085\n",
      "Epoch [882/1000], Loss MSE: 1.2529261112213135, Loss KL: 0.3467855155467987\n",
      "Epoch [883/1000], Loss MSE: 0.7072882652282715, Loss KL: 0.3468511998653412\n",
      "Epoch [884/1000], Loss MSE: 0.6238647699356079, Loss KL: 0.34692463278770447\n",
      "Epoch [885/1000], Loss MSE: 0.3962954878807068, Loss KL: 0.34697869420051575\n",
      "Epoch [886/1000], Loss MSE: 0.5533918142318726, Loss KL: 0.34705716371536255\n",
      "Epoch [887/1000], Loss MSE: 0.6409549117088318, Loss KL: 0.34714123606681824\n",
      "Epoch [888/1000], Loss MSE: 0.7879167795181274, Loss KL: 0.34719815850257874\n",
      "Epoch [889/1000], Loss MSE: 0.28637266159057617, Loss KL: 0.3472959101200104\n",
      "Epoch [890/1000], Loss MSE: 0.4657015800476074, Loss KL: 0.3473708927631378\n",
      "Epoch [891/1000], Loss MSE: 0.6570188999176025, Loss KL: 0.3474954664707184\n",
      "Epoch [892/1000], Loss MSE: 0.5082315802574158, Loss KL: 0.3475474417209625\n",
      "Epoch [893/1000], Loss MSE: 0.23582419753074646, Loss KL: 0.3475980758666992\n",
      "Epoch [894/1000], Loss MSE: 0.3195720613002777, Loss KL: 0.3476409316062927\n",
      "Epoch [895/1000], Loss MSE: 0.5426567792892456, Loss KL: 0.34769001603126526\n",
      "Epoch [896/1000], Loss MSE: 0.4590841829776764, Loss KL: 0.3477749824523926\n",
      "Epoch [897/1000], Loss MSE: 0.36421775817871094, Loss KL: 0.34782126545906067\n",
      "Epoch [898/1000], Loss MSE: 0.9273896217346191, Loss KL: 0.3478759527206421\n",
      "Epoch [899/1000], Loss MSE: 0.14819902181625366, Loss KL: 0.34794142842292786\n",
      "Epoch [900/1000], Loss MSE: 0.2577177882194519, Loss KL: 0.3480074405670166\n",
      "Epoch [901/1000], Loss MSE: 0.5906069278717041, Loss KL: 0.34805920720100403\n",
      "Epoch [902/1000], Loss MSE: 0.9125621914863586, Loss KL: 0.34812480211257935\n",
      "Epoch [903/1000], Loss MSE: 0.3351978361606598, Loss KL: 0.3481820225715637\n",
      "Epoch [904/1000], Loss MSE: 0.5695267915725708, Loss KL: 0.3482556641101837\n",
      "Epoch [905/1000], Loss MSE: 0.8634969592094421, Loss KL: 0.34835708141326904\n",
      "Epoch [906/1000], Loss MSE: 0.33162835240364075, Loss KL: 0.34841617941856384\n",
      "Epoch [907/1000], Loss MSE: 0.30301231145858765, Loss KL: 0.3484596312046051\n",
      "Epoch [908/1000], Loss MSE: 0.40663036704063416, Loss KL: 0.348598837852478\n",
      "Epoch [909/1000], Loss MSE: 0.4987792372703552, Loss KL: 0.34868383407592773\n",
      "Epoch [910/1000], Loss MSE: 0.4196992516517639, Loss KL: 0.3487666845321655\n",
      "Epoch [911/1000], Loss MSE: 0.40185412764549255, Loss KL: 0.3488547205924988\n",
      "Epoch [912/1000], Loss MSE: 1.437902808189392, Loss KL: 0.3489588797092438\n",
      "Epoch [913/1000], Loss MSE: 0.3770992159843445, Loss KL: 0.34900060296058655\n",
      "Epoch [914/1000], Loss MSE: 0.3450021743774414, Loss KL: 0.34909987449645996\n",
      "Epoch [915/1000], Loss MSE: 0.3393135964870453, Loss KL: 0.3491383492946625\n",
      "Epoch [916/1000], Loss MSE: 0.6944869160652161, Loss KL: 0.34920039772987366\n",
      "Epoch [917/1000], Loss MSE: 0.28791117668151855, Loss KL: 0.34927621483802795\n",
      "Epoch [918/1000], Loss MSE: 0.4231485426425934, Loss KL: 0.34936875104904175\n",
      "Epoch [919/1000], Loss MSE: 0.6553559303283691, Loss KL: 0.34939393401145935\n",
      "Epoch [920/1000], Loss MSE: 0.2216857671737671, Loss KL: 0.3494493365287781\n",
      "Epoch [921/1000], Loss MSE: 0.27654939889907837, Loss KL: 0.3495025038719177\n",
      "Epoch [922/1000], Loss MSE: 0.6616784334182739, Loss KL: 0.34952840209007263\n",
      "Epoch [923/1000], Loss MSE: 0.4010074734687805, Loss KL: 0.34957361221313477\n",
      "Epoch [924/1000], Loss MSE: 0.4167931079864502, Loss KL: 0.34963923692703247\n",
      "Epoch [925/1000], Loss MSE: 0.3180222511291504, Loss KL: 0.34970569610595703\n",
      "Epoch [926/1000], Loss MSE: 0.8579082489013672, Loss KL: 0.34974008798599243\n",
      "Epoch [927/1000], Loss MSE: 0.5044000744819641, Loss KL: 0.34980475902557373\n",
      "Epoch [928/1000], Loss MSE: 0.6405879855155945, Loss KL: 0.34988948702812195\n",
      "Epoch [929/1000], Loss MSE: 0.3548879623413086, Loss KL: 0.3499467372894287\n",
      "Epoch [930/1000], Loss MSE: 0.37686437368392944, Loss KL: 0.3499895930290222\n",
      "Epoch [931/1000], Loss MSE: 0.8590379953384399, Loss KL: 0.35005635023117065\n",
      "Epoch [932/1000], Loss MSE: 0.5320795774459839, Loss KL: 0.3500805199146271\n",
      "Epoch [933/1000], Loss MSE: 1.083742380142212, Loss KL: 0.3501439392566681\n",
      "Epoch [934/1000], Loss MSE: 0.7497611045837402, Loss KL: 0.35020074248313904\n",
      "Epoch [935/1000], Loss MSE: 1.1043260097503662, Loss KL: 0.35026872158050537\n",
      "Epoch [936/1000], Loss MSE: 0.777320921421051, Loss KL: 0.35037994384765625\n",
      "Epoch [937/1000], Loss MSE: 0.38921329379081726, Loss KL: 0.35046857595443726\n",
      "Epoch [938/1000], Loss MSE: 0.38495415449142456, Loss KL: 0.35051560401916504\n",
      "Epoch [939/1000], Loss MSE: 0.7078033089637756, Loss KL: 0.3505670130252838\n",
      "Epoch [940/1000], Loss MSE: 0.13832597434520721, Loss KL: 0.35066235065460205\n",
      "Epoch [941/1000], Loss MSE: 0.6108661890029907, Loss KL: 0.3507005572319031\n",
      "Epoch [942/1000], Loss MSE: 0.38064175844192505, Loss KL: 0.35073602199554443\n",
      "Epoch [943/1000], Loss MSE: 0.4478909969329834, Loss KL: 0.3507799804210663\n",
      "Epoch [944/1000], Loss MSE: 0.5757622718811035, Loss KL: 0.35082095861434937\n",
      "Epoch [945/1000], Loss MSE: 0.2418409287929535, Loss KL: 0.35088446736335754\n",
      "Epoch [946/1000], Loss MSE: 0.5177973508834839, Loss KL: 0.35093894600868225\n",
      "Epoch [947/1000], Loss MSE: 0.27792248129844666, Loss KL: 0.3509708046913147\n",
      "Epoch [948/1000], Loss MSE: 0.4911443591117859, Loss KL: 0.3510463833808899\n",
      "Epoch [949/1000], Loss MSE: 0.8312482833862305, Loss KL: 0.3510987460613251\n",
      "Epoch [950/1000], Loss MSE: 0.334415465593338, Loss KL: 0.3511282205581665\n",
      "Epoch [951/1000], Loss MSE: 0.2926414906978607, Loss KL: 0.3511906862258911\n",
      "Epoch [952/1000], Loss MSE: 0.4833528399467468, Loss KL: 0.35121843218803406\n",
      "Epoch [953/1000], Loss MSE: 0.3944917917251587, Loss KL: 0.3512754738330841\n",
      "Epoch [954/1000], Loss MSE: 0.4894375801086426, Loss KL: 0.35132846236228943\n",
      "Epoch [955/1000], Loss MSE: 0.32816746830940247, Loss KL: 0.351365864276886\n",
      "Epoch [956/1000], Loss MSE: 0.4517536163330078, Loss KL: 0.35142549872398376\n",
      "Epoch [957/1000], Loss MSE: 0.7353556752204895, Loss KL: 0.3514680862426758\n",
      "Epoch [958/1000], Loss MSE: 0.45076340436935425, Loss KL: 0.3515743613243103\n",
      "Epoch [959/1000], Loss MSE: 0.42685016989707947, Loss KL: 0.3516407310962677\n",
      "Epoch [960/1000], Loss MSE: 1.1468971967697144, Loss KL: 0.35168829560279846\n",
      "Epoch [961/1000], Loss MSE: 0.5084820985794067, Loss KL: 0.3517516851425171\n",
      "Epoch [962/1000], Loss MSE: 0.25966575741767883, Loss KL: 0.35180628299713135\n",
      "Epoch [963/1000], Loss MSE: 0.49619340896606445, Loss KL: 0.35186368227005005\n",
      "Epoch [964/1000], Loss MSE: 0.7459319829940796, Loss KL: 0.35187986493110657\n",
      "Epoch [965/1000], Loss MSE: 0.701691746711731, Loss KL: 0.3519366383552551\n",
      "Epoch [966/1000], Loss MSE: 0.5345176458358765, Loss KL: 0.351957768201828\n",
      "Epoch [967/1000], Loss MSE: 0.4932493567466736, Loss KL: 0.35201749205589294\n",
      "Epoch [968/1000], Loss MSE: 0.827062726020813, Loss KL: 0.3520706295967102\n",
      "Epoch [969/1000], Loss MSE: 0.7270849943161011, Loss KL: 0.35211193561553955\n",
      "Epoch [970/1000], Loss MSE: 0.6418581008911133, Loss KL: 0.35216593742370605\n",
      "Epoch [971/1000], Loss MSE: 0.6870959997177124, Loss KL: 0.35216259956359863\n",
      "Epoch [972/1000], Loss MSE: 0.3956226408481598, Loss KL: 0.35220515727996826\n",
      "Epoch [973/1000], Loss MSE: 0.3336564600467682, Loss KL: 0.35225462913513184\n",
      "Epoch [974/1000], Loss MSE: 0.48746833205223083, Loss KL: 0.35228899121284485\n",
      "Epoch [975/1000], Loss MSE: 0.232019305229187, Loss KL: 0.35233840346336365\n",
      "Epoch [976/1000], Loss MSE: 0.48016542196273804, Loss KL: 0.3523564636707306\n",
      "Epoch [977/1000], Loss MSE: 0.32800623774528503, Loss KL: 0.35240164399147034\n",
      "Epoch [978/1000], Loss MSE: 0.7675672769546509, Loss KL: 0.35246676206588745\n",
      "Epoch [979/1000], Loss MSE: 0.3705274164676666, Loss KL: 0.35253840684890747\n",
      "Epoch [980/1000], Loss MSE: 0.47027212381362915, Loss KL: 0.352599173784256\n",
      "Epoch [981/1000], Loss MSE: 0.1806257963180542, Loss KL: 0.35263580083847046\n",
      "Epoch [982/1000], Loss MSE: 0.689261794090271, Loss KL: 0.35266372561454773\n",
      "Epoch [983/1000], Loss MSE: 0.3696873188018799, Loss KL: 0.3526900112628937\n",
      "Epoch [984/1000], Loss MSE: 0.3733675181865692, Loss KL: 0.35270074009895325\n",
      "Epoch [985/1000], Loss MSE: 0.5112637877464294, Loss KL: 0.35272425413131714\n",
      "Epoch [986/1000], Loss MSE: 0.5329226851463318, Loss KL: 0.35274362564086914\n",
      "Epoch [987/1000], Loss MSE: 0.3875088095664978, Loss KL: 0.3527451753616333\n",
      "Epoch [988/1000], Loss MSE: 0.25777286291122437, Loss KL: 0.35277438163757324\n",
      "Epoch [989/1000], Loss MSE: 0.7163457274436951, Loss KL: 0.3528003394603729\n",
      "Epoch [990/1000], Loss MSE: 0.2938392758369446, Loss KL: 0.3528263568878174\n",
      "Epoch [991/1000], Loss MSE: 0.3406367897987366, Loss KL: 0.35286080837249756\n",
      "Epoch [992/1000], Loss MSE: 0.23587031662464142, Loss KL: 0.3528742492198944\n",
      "Epoch [993/1000], Loss MSE: 0.37738800048828125, Loss KL: 0.3529239892959595\n",
      "Epoch [994/1000], Loss MSE: 0.5735766887664795, Loss KL: 0.3529967665672302\n",
      "Epoch [995/1000], Loss MSE: 0.6924824714660645, Loss KL: 0.3530018627643585\n",
      "Epoch [996/1000], Loss MSE: 0.6992060542106628, Loss KL: 0.352993369102478\n",
      "Epoch [997/1000], Loss MSE: 0.3856731057167053, Loss KL: 0.3530087471008301\n",
      "Epoch [998/1000], Loss MSE: 0.6233130693435669, Loss KL: 0.35305115580558777\n",
      "Epoch [999/1000], Loss MSE: 0.6813415288925171, Loss KL: 0.3530479073524475\n",
      "Epoch [1000/1000], Loss MSE: 0.35734936594963074, Loss KL: 0.35308316349983215\n",
      "Epoch [1000/1000], total training loss: 0.7104325294494629\n"
     ]
    }
   ],
   "source": [
    "net.train(train_loader, num_epochs=1000, lr=1e-3, verbose=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN_BPP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(BNN_BPP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define the Bayesian neural network architecture based any number of hidden_sizes\n",
    "        self.fc_mu = nn.ModuleList([nn.Linear(input_size, hidden_sizes[0])])\n",
    "\n",
    "        for i in range(len(hidden_sizes) - 1):   #adding  layers for mu and log_var\n",
    "            self.fc_mu.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        self.fc_mu.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    # Define loss function (variational lower bound)\n",
    "    def variational_loss(self, output_mu, output_log_var, target):\n",
    "        recon_loss = F.mse_loss(output_mu, target, reduction='mean')\n",
    "        kl_divergence = -0.5 * torch.sum(1 + output_log_var - output_mu.pow(2) - output_log_var.exp())\n",
    "        return recon_loss , kl_divergence\n",
    "    \n",
    "    def train(self, train_loader,num_epochs=10, lr=1e-3, verbose=0):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, targets in train_loader:       \n",
    "                optimizer.zero_grad()\n",
    "                output_mu, output_log_var = self(inputs)\n",
    "                loss_recons, kl_loss = self.variational_loss(output_mu, output_log_var, targets)\n",
    "                loss = loss_recons + kl_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if not verbose == 0: \n",
    "                # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss MSE: {loss_recons}, Loss KL: {kl_loss}')\n",
    "            \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], total loss: {loss.item()}')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        hidden_mu = F.relu(self.fc_mu[0](x))\n",
    "        hidden_log_var = F.relu(self.fc_log_var[0](x))\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        hidden_rep = self.reparameterize(hidden_mu, hidden_log_var)\n",
    "        \n",
    "        # Decoder\n",
    "        for i in range(1, len(self.fc_mu)-1):   #predictions with Reparameterization over all layers but last one (modified)\n",
    "            # print(hidden_rep.shape, i)\n",
    "            hidden_mu = F.relu(self.fc_mu[i](hidden_rep))\n",
    "            hidden_log_var = F.relu(self.fc_log_var[i](hidden_rep))\n",
    "            hidden_rep = self.reparameterize(hidden_mu, hidden_log_var)\n",
    "            # print(hidden_rep.shape, i)\n",
    "        \n",
    "        output_mu = self.fc_mu[-1](hidden_rep)\n",
    "        output_log_var = self.fc_log_var[-1](hidden_rep)\n",
    "        \n",
    "        return output_mu, output_log_var"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# #range for sampling\n",
    "# points_plot=1000                              #resolution of the ground truth \n",
    "# x_plot=np.linspace(xmin,xmax,points_plot) \n",
    "# y_plot=function(x_plot)\n",
    "\n",
    "# x_train_net = torch.from_numpy(x_plot.astype(np.float32)).view(-1,1)\n",
    "# y_bnn = net.predict(x_train_net).cpu().detach().numpy()\n",
    "# y_bnn_pred = net.sample_predict(x_train_net, Nsamples=10).cpu().detach().numpy()\n",
    "# y_bnn_mean = np.mean(y_bnn_pred, axis=0)\n",
    "# y_bnn_std = np.std(y_bnn_pred, axis=0)\n",
    "\n",
    "# fig = plt.figure(figsize=(10,5))\n",
    "# ax=fig.add_axes([0,0,1,1])\n",
    "# ax.plot(x_plot, y_plot,label=r\"Ground truth\",linestyle='dashed',linewidth=0.5,color='k')\n",
    "# ax.plot(x_plot, y_bnn,label=r\"BNN\",linestyle='dashed',linewidth=1,color='b')\n",
    "# ax.plot(x_plot, y_bnn_mean,label=r\"BNN_mean\",linestyle='dashed',linewidth=0.5,color='r')\n",
    "# ax.plot(x_plot, y_bnn_mean+y_bnn_std,label=r\"BNN_std\",linestyle='dashed',linewidth=0.5,color='g')\n",
    "# ax.plot(x_plot, y_bnn_mean-y_bnn_std,label=r\"BNN_std\",linestyle='dashed',linewidth=0.5,color='g')\n",
    "# # ax.scatter(x_train.cpu().detach().numpy(), y_train.cpu().detach().numpy())\n",
    "# # ax.scatter(t2,PC,label=r\"Predictions\",color='g',s=10)\n",
    "# plt.scatter(xn, M, label=\"Observations\",marker=\"x\",color='r',s=200)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "class BNN_BPP(nn.Module):\n",
    "    def __init__(self, input_size, hidden_sizes, output_size):\n",
    "        super(BNN_BPP, self).__init__()\n",
    "        self.input_size = input_size\n",
    "        self.hidden_sizes = hidden_sizes\n",
    "        self.output_size = output_size\n",
    "        \n",
    "        # Define the Bayesian neural network architecture based any number of hidden_sizes\n",
    "        self.fc_mu = nn.ModuleList([nn.Linear(input_size, hidden_sizes[0])])\n",
    "        self.fc_log_var = nn.ModuleList([nn.Linear(input_size, hidden_sizes[0])])\n",
    "        for i in range(len(hidden_sizes) - 1):   #adding  layers for mu and log_var\n",
    "            self.fc_mu.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "            self.fc_log_var.append(nn.Linear(hidden_sizes[i], hidden_sizes[i+1]))\n",
    "        self.fc_mu.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        self.fc_log_var.append(nn.Linear(hidden_sizes[-1], output_size))\n",
    "        \n",
    "    def reparameterize(self, mu, log_var):\n",
    "        std = torch.exp(0.5 * log_var)\n",
    "        eps = torch.randn_like(std)\n",
    "        return mu + eps * std\n",
    "    \n",
    "    # Define loss function (variational lower bound)\n",
    "    def variational_loss(self, output_mu, output_log_var, target):\n",
    "        recon_loss = F.mse_loss(output_mu, target, reduction='mean')\n",
    "        kl_divergence = -0.5 * torch.sum(1 + output_log_var - output_mu.pow(2) - output_log_var.exp())\n",
    "        return recon_loss , kl_divergence\n",
    "    \n",
    "    def train(self, train_loader,num_epochs=10, lr=1e-3, verbose=0):\n",
    "        optimizer = torch.optim.Adam(self.parameters(), lr=lr)\n",
    "\n",
    "        for epoch in range(num_epochs):\n",
    "            for inputs, targets in train_loader:       \n",
    "                optimizer.zero_grad()\n",
    "                output_mu, output_log_var = self(inputs)\n",
    "                loss_recons, kl_loss = self.variational_loss(output_mu, output_log_var, targets)\n",
    "                loss = loss_recons + kl_loss\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "\n",
    "            if not verbose == 0: \n",
    "                # print(f'Epoch [{epoch+1}/{num_epochs}], Loss: {loss.item()}')\n",
    "                print(f'Epoch [{epoch+1}/{num_epochs}], Loss MSE: {loss_recons}, Loss KL: {kl_loss}')\n",
    "            \n",
    "        print(f'Epoch [{epoch+1}/{num_epochs}], total loss: {loss.item()}')\n",
    "        \n",
    "    def forward(self, x):\n",
    "        # Encoder\n",
    "        hidden_mu = F.relu(self.fc_mu[0](x))\n",
    "        hidden_log_var = F.relu(self.fc_log_var[0](x))\n",
    "        \n",
    "        # Reparameterization trick\n",
    "        hidden_rep = self.reparameterize(hidden_mu, hidden_log_var)\n",
    "        \n",
    "        # Decoder\n",
    "        for i in range(1, len(self.fc_mu)-1):   #predictions with Reparameterization over all layers but last one (modified)\n",
    "            # print(hidden_rep.shape, i)\n",
    "            hidden_mu = F.relu(self.fc_mu[i](hidden_rep))\n",
    "            hidden_log_var = F.relu(self.fc_log_var[i](hidden_rep))\n",
    "            hidden_rep = self.reparameterize(hidden_mu, hidden_log_var)\n",
    "            # print(hidden_rep.shape, i)\n",
    "        \n",
    "        output_mu = self.fc_mu[-1](hidden_rep)\n",
    "        output_log_var = self.fc_log_var[-1](hidden_rep)\n",
    "        \n",
    "        return output_mu, output_log_var"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "deep_learning",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
